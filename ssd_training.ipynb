{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4090\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# ---------- edit ---------- #\n",
    "print(torch.cuda.get_device_name(torch.device('cuda')))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "# device = torch.device('cpu')\n",
    "print(device)\n",
    "# -------------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "from truth_voc import make_filepath_list, GetBBoxAndLabel, DataTransform, multiobject_collate_fn\n",
    "from truth_voc import PreprocessVOC2012\n",
    "\n",
    "# rootpath = '/home/masakibandai/object_detection/data/VOCdevkit/VOC2012/'\n",
    "rootpath = '/Users/ShimaSef/object_detection/data/VOCdevkit/VOC2012/'\n",
    "train_img_list, train_anno_list, val_img_list, val_anno_list = make_filepath_list(rootpath)\n",
    "voc_classes = [\n",
    "    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', \n",
    "    'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n",
    "]\n",
    "\n",
    "color_mean = (104, 117, 123)\n",
    "input_size = 300\n",
    "train_dataset = PreprocessVOC2012(\n",
    "    train_img_list,\n",
    "    train_anno_list,\n",
    "    phase='train',\n",
    "    transform=DataTransform(input_size, color_mean),\n",
    "    get_bbox_label=GetBBoxAndLabel(voc_classes)\n",
    ")\n",
    "\n",
    "val_dataset = PreprocessVOC2012(\n",
    "    val_img_list,\n",
    "    val_anno_list,\n",
    "    phase='val',\n",
    "    transform=DataTransform(input_size, color_mean),\n",
    "    get_bbox_label=GetBBoxAndLabel(voc_classes)\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataloader = data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=multiobject_collate_fn\n",
    ")\n",
    "\n",
    "val_dataloader = data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=multiobject_collate_fn\n",
    ")\n",
    "\n",
    "dataloaders_dict = {'train': train_dataloader, 'val': val_dataloader}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
       "===================================================================================================================\n",
       "SSD                                      [32, 3, 300, 300]         [32, 8732, 4]             --\n",
       "├─ModuleList: 1-3                        --                        --                        (recursive)\n",
       "│    └─Conv2d: 2-1                       [32, 3, 300, 300]         [32, 64, 300, 300]        1,792\n",
       "│    └─ReLU: 2-2                         [32, 64, 300, 300]        [32, 64, 300, 300]        --\n",
       "│    └─Conv2d: 2-3                       [32, 64, 300, 300]        [32, 64, 300, 300]        36,928\n",
       "│    └─ReLU: 2-4                         [32, 64, 300, 300]        [32, 64, 300, 300]        --\n",
       "│    └─MaxPool2d: 2-5                    [32, 64, 300, 300]        [32, 64, 150, 150]        --\n",
       "│    └─Conv2d: 2-6                       [32, 64, 150, 150]        [32, 128, 150, 150]       73,856\n",
       "│    └─ReLU: 2-7                         [32, 128, 150, 150]       [32, 128, 150, 150]       --\n",
       "│    └─Conv2d: 2-8                       [32, 128, 150, 150]       [32, 128, 150, 150]       147,584\n",
       "│    └─ReLU: 2-9                         [32, 128, 150, 150]       [32, 128, 150, 150]       --\n",
       "│    └─MaxPool2d: 2-10                   [32, 128, 150, 150]       [32, 128, 75, 75]         --\n",
       "│    └─Conv2d: 2-11                      [32, 128, 75, 75]         [32, 256, 75, 75]         295,168\n",
       "│    └─ReLU: 2-12                        [32, 256, 75, 75]         [32, 256, 75, 75]         --\n",
       "│    └─Conv2d: 2-13                      [32, 256, 75, 75]         [32, 256, 75, 75]         590,080\n",
       "│    └─ReLU: 2-14                        [32, 256, 75, 75]         [32, 256, 75, 75]         --\n",
       "│    └─Conv2d: 2-15                      [32, 256, 75, 75]         [32, 256, 75, 75]         590,080\n",
       "│    └─ReLU: 2-16                        [32, 256, 75, 75]         [32, 256, 75, 75]         --\n",
       "│    └─MaxPool2d: 2-17                   [32, 256, 75, 75]         [32, 256, 38, 38]         --\n",
       "│    └─Conv2d: 2-18                      [32, 256, 38, 38]         [32, 512, 38, 38]         1,180,160\n",
       "│    └─ReLU: 2-19                        [32, 512, 38, 38]         [32, 512, 38, 38]         --\n",
       "│    └─Conv2d: 2-20                      [32, 512, 38, 38]         [32, 512, 38, 38]         2,359,808\n",
       "│    └─ReLU: 2-21                        [32, 512, 38, 38]         [32, 512, 38, 38]         --\n",
       "│    └─Conv2d: 2-22                      [32, 512, 38, 38]         [32, 512, 38, 38]         2,359,808\n",
       "│    └─ReLU: 2-23                        [32, 512, 38, 38]         [32, 512, 38, 38]         --\n",
       "├─L2Norm: 1-2                            [32, 512, 38, 38]         [32, 512, 38, 38]         512\n",
       "├─ModuleList: 1-3                        --                        --                        (recursive)\n",
       "│    └─MaxPool2d: 2-24                   [32, 512, 38, 38]         [32, 512, 19, 19]         --\n",
       "│    └─Conv2d: 2-25                      [32, 512, 19, 19]         [32, 512, 19, 19]         2,359,808\n",
       "│    └─ReLU: 2-26                        [32, 512, 19, 19]         [32, 512, 19, 19]         --\n",
       "│    └─Conv2d: 2-27                      [32, 512, 19, 19]         [32, 512, 19, 19]         2,359,808\n",
       "│    └─ReLU: 2-28                        [32, 512, 19, 19]         [32, 512, 19, 19]         --\n",
       "│    └─Conv2d: 2-29                      [32, 512, 19, 19]         [32, 512, 19, 19]         2,359,808\n",
       "│    └─ReLU: 2-30                        [32, 512, 19, 19]         [32, 512, 19, 19]         --\n",
       "│    └─MaxPool2d: 2-31                   [32, 512, 19, 19]         [32, 512, 19, 19]         --\n",
       "│    └─Conv2d: 2-32                      [32, 512, 19, 19]         [32, 1024, 19, 19]        4,719,616\n",
       "│    └─ReLU: 2-33                        [32, 1024, 19, 19]        [32, 1024, 19, 19]        --\n",
       "│    └─Conv2d: 2-34                      [32, 1024, 19, 19]        [32, 1024, 19, 19]        1,049,600\n",
       "│    └─ReLU: 2-35                        [32, 1024, 19, 19]        [32, 1024, 19, 19]        --\n",
       "├─ModuleList: 1-4                        --                        --                        --\n",
       "│    └─Conv2d: 2-36                      [32, 1024, 19, 19]        [32, 256, 19, 19]         262,400\n",
       "│    └─Conv2d: 2-37                      [32, 256, 19, 19]         [32, 512, 10, 10]         1,180,160\n",
       "│    └─Conv2d: 2-38                      [32, 512, 10, 10]         [32, 128, 10, 10]         65,664\n",
       "│    └─Conv2d: 2-39                      [32, 128, 10, 10]         [32, 256, 5, 5]           295,168\n",
       "│    └─Conv2d: 2-40                      [32, 256, 5, 5]           [32, 128, 5, 5]           32,896\n",
       "│    └─Conv2d: 2-41                      [32, 128, 5, 5]           [32, 256, 3, 3]           295,168\n",
       "│    └─Conv2d: 2-42                      [32, 256, 3, 3]           [32, 128, 3, 3]           32,896\n",
       "│    └─Conv2d: 2-43                      [32, 128, 3, 3]           [32, 256, 1, 1]           295,168\n",
       "├─ModuleList: 1-15                       --                        --                        (recursive)\n",
       "│    └─Conv2d: 2-44                      [32, 512, 38, 38]         [32, 16, 38, 38]          73,744\n",
       "├─ModuleList: 1-16                       --                        --                        (recursive)\n",
       "│    └─Conv2d: 2-45                      [32, 512, 38, 38]         [32, 84, 38, 38]          387,156\n",
       "├─ModuleList: 1-15                       --                        --                        (recursive)\n",
       "│    └─Conv2d: 2-46                      [32, 1024, 19, 19]        [32, 24, 19, 19]          221,208\n",
       "├─ModuleList: 1-16                       --                        --                        (recursive)\n",
       "│    └─Conv2d: 2-47                      [32, 1024, 19, 19]        [32, 126, 19, 19]         1,161,342\n",
       "├─ModuleList: 1-15                       --                        --                        (recursive)\n",
       "│    └─Conv2d: 2-48                      [32, 512, 10, 10]         [32, 24, 10, 10]          110,616\n",
       "├─ModuleList: 1-16                       --                        --                        (recursive)\n",
       "│    └─Conv2d: 2-49                      [32, 512, 10, 10]         [32, 126, 10, 10]         580,734\n",
       "├─ModuleList: 1-15                       --                        --                        (recursive)\n",
       "│    └─Conv2d: 2-50                      [32, 256, 5, 5]           [32, 24, 5, 5]            55,320\n",
       "├─ModuleList: 1-16                       --                        --                        (recursive)\n",
       "│    └─Conv2d: 2-51                      [32, 256, 5, 5]           [32, 126, 5, 5]           290,430\n",
       "├─ModuleList: 1-15                       --                        --                        (recursive)\n",
       "│    └─Conv2d: 2-52                      [32, 256, 3, 3]           [32, 16, 3, 3]            36,880\n",
       "├─ModuleList: 1-16                       --                        --                        (recursive)\n",
       "│    └─Conv2d: 2-53                      [32, 256, 3, 3]           [32, 84, 3, 3]            193,620\n",
       "├─ModuleList: 1-15                       --                        --                        (recursive)\n",
       "│    └─Conv2d: 2-54                      [32, 256, 1, 1]           [32, 16, 1, 1]            36,880\n",
       "├─ModuleList: 1-16                       --                        --                        (recursive)\n",
       "│    └─Conv2d: 2-55                      [32, 256, 1, 1]           [32, 84, 1, 1]            193,620\n",
       "===================================================================================================================\n",
       "Total params: 26,285,486\n",
       "Trainable params: 26,285,486\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (T): 1.00\n",
       "===================================================================================================================\n",
       "Input size (MB): 34.56\n",
       "Forward/backward pass size (MB): 6717.23\n",
       "Params size (MB): 105.14\n",
       "Estimated Total Size (MB): 6856.93\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from truth_ssd import SSD\n",
    "from torchinfo import summary\n",
    "\n",
    "ssd_cfg = {\n",
    "    'classes_num': 21,\n",
    "    'input_size': 300,\n",
    "    'dbox_num': [4, 6, 6, 6, 4, 4],\n",
    "    'feature_maps': [38, 19, 10, 5, 3, 1],\n",
    "    'steps': [8, 16, 32, 64, 100, 300],\n",
    "    'min_sizes': [30, 60, 111, 162, 213, 264],\n",
    "    'max_sizes': [60, 111, 162, 213, 264, 315],\n",
    "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "}\n",
    "\n",
    "net = SSD(phase='train', cfg=ssd_cfg)\n",
    "# weightpath = '/home/masakibandai/object_detection/weights/vgg16_reducedfc.pth'\n",
    "weightpath = '/Users/ShimaSef/object_detection/weights/vgg16_reducedfc.pth'\n",
    "vgg_weights = torch.load(weightpath)\n",
    "net.vgg.load_state_dict(vgg_weights)\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "net.extras.apply(weights_init)\n",
    "net.loc.apply(weights_init)\n",
    "net.conf.apply(weights_init)\n",
    "\n",
    "summary(\n",
    "    net,\n",
    "    input_size=(batch_size, 3, 300, 300),\n",
    "    col_names=['input_size', 'output_size', 'num_params']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from truth_ssd import MultiBoxLoss\n",
    "\n",
    "criterion = MultiBoxLoss(\n",
    "    jaccard_thresh=0.5,\n",
    "    neg_pos=3,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# ---------- edit ---------- #\n",
    "optimizer = optim.Adam(\n",
    "    net.parameters(),\n",
    "    lr=1e-3,\n",
    "    # momentum=0.9,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-08,\n",
    "    weight_decay=0,\n",
    "    amsgrad=False\n",
    ")\n",
    "# -------------------------- #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def train(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "    '''\n",
    "    Parameter:\n",
    "        net(object): SSD model\n",
    "        dataloaders_dict(dict of object): dataloader\n",
    "        criterion(object): loss function\n",
    "        optimizer(object): optimizer\n",
    "        num_epochs(object): num learning\n",
    "    '''\n",
    "    print(device)\n",
    "    # ---------edit--------- #\n",
    "    net.to(device)\n",
    "    # ---------------------- #\n",
    "    print('Start training with {}'.format(torch.cuda.get_device_name()))\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    iteration = 1\n",
    "    epoch_train_loss = 0.0\n",
    "    epoch_val_loss = 0.0\n",
    "    logs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        t_epoch_start = time.time()\n",
    "        t_iter_start = time.time()\n",
    "\n",
    "        print('----------------------------------------------------------------------')\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('----------------------------------------------------------------------')\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()\n",
    "            else:\n",
    "                if ((epoch+1) % 10 == 0):\n",
    "                    net.eval()\n",
    "                    print('----------------------------------------------------------------------')\n",
    "                    print('(validation)')\n",
    "                else:\n",
    "                    continue\n",
    "        \n",
    "            for images, targets in dataloaders_dict[phase]:\n",
    "                images = images.to(device)\n",
    "                targets = [ann.to(device) for ann in targets]\n",
    "                # ---------edit---------- #\n",
    "                # images.cuda()\n",
    "                # targets.cuda()\n",
    "                # ----------------------- #\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase=='train'):\n",
    "                    outputs = net(images)\n",
    "                    loss_l, loss_c = criterion(outputs, targets)\n",
    "                    loss = loss_l + loss_c\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        nn.utils.clip_grad_value_(net.parameters(), clip_value=2.0)\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        if (iteration % 10 == 0):\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            print('Step({}) loss: {:.4f} -- time: {:.4f} sec.'.format(iteration, loss.item(), duration))\n",
    "                            t_iter_start = time.time()\n",
    "                        \n",
    "                        epoch_train_loss += loss.item()\n",
    "                        iteration += 1\n",
    "\n",
    "                    else:\n",
    "                        epoch_val_loss += loss.item()\n",
    "        t_epoch_finish = time.time()\n",
    "        print('----------------------------------------------------------------------')\n",
    "\n",
    "        print('train_loss: {:.4f} - val_loss: {:.4f}'.format(epoch_train_loss, epoch_val_loss))\n",
    "        print('time: {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
    "        print('time_left: {:.4f} min'.format((t_epoch_finish-t_epoch_start)*(num_epochs-epoch-1)/60))\n",
    "\n",
    "        t_epoch_start = time.time()\n",
    "        log_epoch = {\n",
    "            'epoch': epoch+1,\n",
    "            'train_loss': epoch_train_loss,\n",
    "            'val_loss': epoch_val_loss\n",
    "        }\n",
    "\n",
    "        logs.append(log_epoch)\n",
    "        df = pd.DataFrame(logs)\n",
    "        # ---------- edit ---------- #\n",
    "        # csvpath = '/home/masakibandai/object_detection/epoch_loss.csv'\n",
    "        if epoch == 0:\n",
    "            csvidx = os.listdir('loss_csv')\n",
    "            if not os.path.exists('./loss_csv/epoch_loss_0.csv'):\n",
    "                csvpath = './loss_csv/epoch_loss_0.csv'\n",
    "            else:\n",
    "                csvidx = [int(os.path.splitext(i)[0].split('_')[2]) for i in csvidx]\n",
    "                csvpath = '/Users/ShimaSef/object_detection/loss_csv/epoch_loss_{}.csv'.format(max(csvidx)+1)\n",
    "        # -------------------------- #\n",
    "        df.to_csv(csvpath)\n",
    "        epoch_train_loss = 0.0\n",
    "        epoch_val_loss = 0.0\n",
    "        \n",
    "        # ---------- edit ---------- #\n",
    "        # statedictpath = '/home/masakibandai/object_detection/weights/ssd_weights'\n",
    "        if epoch == 0:\n",
    "            weightidx = os.listdir('weights')\n",
    "            if not os.path.exists('./weights/ssd_weights_0/'):\n",
    "                os.mkdir('./weights/ssd_weights_0/')\n",
    "                statedictpath = '/Users/ShimaSef/object_detection/weights/ssd_weights_0/ssd_weights_'\n",
    "            else:\n",
    "                weightidx = [int(f.split('_')[2]) for f in weightidx if os.path.isdir(os.path.join('weights', f))]\n",
    "                if not os.path.exists('/Users/ShimaSef/object_detection/weights/ssd_weights_{}/'.format(max(weightidx)+1)):\n",
    "                    os.mkdir('/Users/ShimaSef/object_detection/weights/ssd_weights_{}/'.format(max(weightidx)+1))\n",
    "                statedictpath = '/Users/ShimaSef/object_detection/weights/ssd_weights_{}/ssd_weights_'.format(max(weightidx)+1)\n",
    "        # -------------------------- #\n",
    "\n",
    "        if ((epoch+1) % 10 == 0):\n",
    "            torch.save(\n",
    "                net.state_dict(),\n",
    "                 statedictpath + str(epoch+1) + '.pth'\n",
    "            )\n",
    "            print('--saved weights--')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Start training with NVIDIA GeForce RTX 4090\n",
      "----------------------------------------------------------------------\n",
      "Epoch 1/200\n",
      "----------------------------------------------------------------------\n",
      "Step(10) loss: 20.5678 -- time: 10.8081 sec.\n",
      "Step(20) loss: 14.1930 -- time: 4.2875 sec.\n",
      "Step(30) loss: 12.9150 -- time: 4.3424 sec.\n",
      "Step(40) loss: 10.7369 -- time: 4.3773 sec.\n",
      "Step(50) loss: 30.7338 -- time: 4.4570 sec.\n",
      "Step(60) loss: 7.9934 -- time: 4.4204 sec.\n",
      "Step(70) loss: 14.8957 -- time: 4.5108 sec.\n",
      "Step(80) loss: 8.5588 -- time: 4.3118 sec.\n",
      "Step(90) loss: 11.2703 -- time: 4.4122 sec.\n",
      "Step(100) loss: 9.9506 -- time: 4.2711 sec.\n",
      "Step(110) loss: 8.2822 -- time: 4.3992 sec.\n",
      "Step(120) loss: 8.0350 -- time: 4.2385 sec.\n",
      "Step(130) loss: 7.3107 -- time: 4.4095 sec.\n",
      "Step(140) loss: 12.2808 -- time: 4.3179 sec.\n",
      "Step(150) loss: 7.3972 -- time: 4.5186 sec.\n",
      "Step(160) loss: 14.3432 -- time: 4.4403 sec.\n",
      "Step(170) loss: 8.2793 -- time: 4.4199 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 3600.6443 - val_loss: 0.0000\n",
      "time: 92.9721 sec.\n",
      "time_left: 308.3573 min\n",
      "----------------------------------------------------------------------\n",
      "Epoch 2/200\n",
      "----------------------------------------------------------------------\n",
      "Step(180) loss: 4.5153 -- time: 0.4035 sec.\n",
      "Step(190) loss: 10.0923 -- time: 4.3208 sec.\n",
      "Step(200) loss: 8.8396 -- time: 4.2897 sec.\n",
      "Step(210) loss: 11.4394 -- time: 4.2514 sec.\n",
      "Step(220) loss: 8.4218 -- time: 4.3790 sec.\n",
      "Step(230) loss: 10.2717 -- time: 4.2582 sec.\n",
      "Step(240) loss: 9.2381 -- time: 4.0659 sec.\n",
      "Step(250) loss: 9.4647 -- time: 4.3084 sec.\n",
      "Step(260) loss: 6.3700 -- time: 4.2523 sec.\n",
      "Step(270) loss: 10.2003 -- time: 4.2986 sec.\n",
      "Step(280) loss: 6.9592 -- time: 4.2751 sec.\n",
      "Step(290) loss: 10.8694 -- time: 4.1491 sec.\n",
      "Step(300) loss: 7.7375 -- time: 4.3375 sec.\n",
      "Step(310) loss: 6.0748 -- time: 4.3200 sec.\n",
      "Step(320) loss: 8.1909 -- time: 4.3674 sec.\n",
      "Step(330) loss: 11.9057 -- time: 4.2532 sec.\n",
      "Step(340) loss: 6.7301 -- time: 4.4054 sec.\n",
      "Step(350) loss: 9.3843 -- time: 4.2658 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 1636.6849 - val_loss: 0.0000\n",
      "time: 78.3020 sec.\n",
      "time_left: 258.3965 min\n",
      "----------------------------------------------------------------------\n",
      "Epoch 3/200\n",
      "----------------------------------------------------------------------\n",
      "Step(360) loss: 8.8078 -- time: 0.7857 sec.\n",
      "Step(370) loss: 6.8659 -- time: 4.3440 sec.\n",
      "Step(380) loss: 7.9426 -- time: 4.2209 sec.\n",
      "Step(390) loss: 8.9884 -- time: 4.4470 sec.\n",
      "Step(400) loss: 5.2337 -- time: 4.3457 sec.\n",
      "Step(410) loss: 8.2011 -- time: 4.1964 sec.\n",
      "Step(420) loss: 9.4767 -- time: 4.2440 sec.\n",
      "Step(430) loss: 8.6884 -- time: 4.2407 sec.\n",
      "Step(440) loss: 6.2081 -- time: 4.3804 sec.\n",
      "Step(450) loss: 8.1255 -- time: 4.2476 sec.\n",
      "Step(460) loss: 11.1491 -- time: 4.2356 sec.\n",
      "Step(470) loss: 9.5336 -- time: 4.1242 sec.\n",
      "Step(480) loss: 9.2814 -- time: 4.2736 sec.\n",
      "Step(490) loss: 9.5669 -- time: 4.2530 sec.\n",
      "Step(500) loss: 8.2870 -- time: 4.5760 sec.\n",
      "Step(510) loss: 9.8160 -- time: 4.7112 sec.\n",
      "Step(520) loss: 8.1114 -- time: 5.1158 sec.\n",
      "Step(530) loss: 16.2336 -- time: 4.3561 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 1512.6220 - val_loss: 0.0000\n",
      "time: 79.9914 sec.\n",
      "time_left: 262.6384 min\n",
      "----------------------------------------------------------------------\n",
      "Epoch 4/200\n",
      "----------------------------------------------------------------------\n",
      "Step(540) loss: 8.2178 -- time: 1.1523 sec.\n",
      "Step(550) loss: 9.2908 -- time: 4.3342 sec.\n",
      "Step(560) loss: 10.4725 -- time: 4.3591 sec.\n",
      "Step(570) loss: 8.0506 -- time: 4.2474 sec.\n",
      "Step(580) loss: 5.9278 -- time: 4.3240 sec.\n",
      "Step(590) loss: 8.9881 -- time: 4.3743 sec.\n",
      "Step(600) loss: 8.0303 -- time: 4.2937 sec.\n",
      "Step(610) loss: 21.8198 -- time: 4.3235 sec.\n",
      "Step(620) loss: 7.2157 -- time: 4.4206 sec.\n",
      "Step(630) loss: 5.4082 -- time: 4.2820 sec.\n",
      "Step(640) loss: 8.5478 -- time: 4.3896 sec.\n",
      "Step(650) loss: 7.7884 -- time: 4.2760 sec.\n",
      "Step(660) loss: 10.7579 -- time: 4.3666 sec.\n",
      "Step(670) loss: 8.6233 -- time: 4.2432 sec.\n",
      "Step(680) loss: 8.8237 -- time: 4.4589 sec.\n",
      "Step(690) loss: 8.4971 -- time: 4.1533 sec.\n",
      "Step(700) loss: 7.9203 -- time: 4.1322 sec.\n",
      "Step(710) loss: 9.4500 -- time: 4.2774 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 1555.7804 - val_loss: 0.0000\n",
      "time: 78.7918 sec.\n",
      "time_left: 257.3865 min\n",
      "----------------------------------------------------------------------\n",
      "Epoch 5/200\n",
      "----------------------------------------------------------------------\n",
      "Step(720) loss: 8.8111 -- time: 1.6565 sec.\n",
      "Step(730) loss: 9.8767 -- time: 4.3193 sec.\n",
      "Step(740) loss: 8.3450 -- time: 4.3325 sec.\n",
      "Step(750) loss: 8.6908 -- time: 4.3293 sec.\n",
      "Step(760) loss: 8.5341 -- time: 4.5396 sec.\n",
      "Step(770) loss: 7.8022 -- time: 4.4497 sec.\n",
      "Step(780) loss: 8.7905 -- time: 4.3380 sec.\n",
      "Step(790) loss: 7.3556 -- time: 4.2186 sec.\n",
      "Step(800) loss: 16.8840 -- time: 4.2879 sec.\n",
      "Step(810) loss: 12.5077 -- time: 4.2899 sec.\n",
      "Step(820) loss: 17.0398 -- time: 4.1752 sec.\n",
      "Step(830) loss: 11.9620 -- time: 4.1833 sec.\n",
      "Step(840) loss: 8.7047 -- time: 4.1618 sec.\n",
      "Step(850) loss: 8.7814 -- time: 4.3337 sec.\n",
      "Step(860) loss: 13.7611 -- time: 4.2101 sec.\n",
      "Step(870) loss: 9.3552 -- time: 4.3113 sec.\n",
      "Step(880) loss: 8.3583 -- time: 4.2562 sec.\n",
      "Step(890) loss: 7.6878 -- time: 4.4503 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 2437.4654 - val_loss: 0.0000\n",
      "time: 78.7378 sec.\n",
      "time_left: 255.8977 min\n",
      "----------------------------------------------------------------------\n",
      "Epoch 6/200\n",
      "----------------------------------------------------------------------\n",
      "Step(900) loss: 5.2292 -- time: 2.0270 sec.\n",
      "Step(910) loss: 12.9805 -- time: 4.2954 sec.\n",
      "Step(920) loss: 8.0369 -- time: 4.3212 sec.\n",
      "Step(930) loss: 13.7454 -- time: 4.3985 sec.\n",
      "Step(940) loss: 6.4308 -- time: 4.3750 sec.\n",
      "Step(950) loss: 12.2872 -- time: 4.1953 sec.\n",
      "Step(960) loss: 10.4447 -- time: 4.3511 sec.\n",
      "Step(970) loss: 6.6920 -- time: 4.2334 sec.\n",
      "Step(980) loss: 6.2480 -- time: 4.2741 sec.\n",
      "Step(990) loss: 9.3331 -- time: 4.4519 sec.\n",
      "Step(1000) loss: 5.6403 -- time: 4.2277 sec.\n",
      "Step(1010) loss: 8.8581 -- time: 4.3126 sec.\n",
      "Step(1020) loss: 15.0790 -- time: 4.3225 sec.\n",
      "Step(1030) loss: 10.7311 -- time: 4.3454 sec.\n",
      "Step(1040) loss: 8.6908 -- time: 4.2942 sec.\n",
      "Step(1050) loss: 19.1809 -- time: 4.2909 sec.\n",
      "Step(1060) loss: 11.2052 -- time: 4.1535 sec.\n",
      "Step(1070) loss: 11.1610 -- time: 4.2953 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 1656.4923 - val_loss: 0.0000\n",
      "time: 78.7530 sec.\n",
      "time_left: 254.6348 min\n",
      "----------------------------------------------------------------------\n",
      "Epoch 7/200\n",
      "----------------------------------------------------------------------\n",
      "Step(1080) loss: 4.4373 -- time: 2.5745 sec.\n",
      "Step(1090) loss: 8.7786 -- time: 4.4551 sec.\n",
      "Step(1100) loss: 7.9318 -- time: 4.1763 sec.\n",
      "Step(1110) loss: 8.9790 -- time: 4.4615 sec.\n",
      "Step(1120) loss: 11.8575 -- time: 4.3048 sec.\n",
      "Step(1130) loss: 9.1586 -- time: 4.4030 sec.\n",
      "Step(1140) loss: 12.3755 -- time: 4.3472 sec.\n",
      "Step(1150) loss: 6.5922 -- time: 4.4526 sec.\n",
      "Step(1160) loss: 6.1740 -- time: 4.2390 sec.\n",
      "Step(1170) loss: 9.5895 -- time: 4.3721 sec.\n",
      "Step(1180) loss: 5.9400 -- time: 4.1607 sec.\n",
      "Step(1190) loss: 7.4827 -- time: 4.3178 sec.\n",
      "Step(1200) loss: 6.4410 -- time: 4.3369 sec.\n",
      "Step(1210) loss: 7.0792 -- time: 4.2850 sec.\n",
      "Step(1220) loss: 6.4344 -- time: 4.3640 sec.\n",
      "Step(1230) loss: 5.5627 -- time: 4.6082 sec.\n",
      "Step(1240) loss: 9.3730 -- time: 4.2241 sec.\n",
      "Step(1250) loss: 5.8655 -- time: 4.1236 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 1561.0995 - val_loss: 0.0000\n",
      "time: 79.2080 sec.\n",
      "time_left: 254.7857 min\n",
      "----------------------------------------------------------------------\n",
      "Epoch 8/200\n",
      "----------------------------------------------------------------------\n",
      "Step(1260) loss: 11.8426 -- time: 2.9713 sec.\n",
      "Step(1270) loss: 4.9886 -- time: 4.2787 sec.\n",
      "Step(1280) loss: 14.4243 -- time: 4.3146 sec.\n",
      "Step(1290) loss: 9.5276 -- time: 4.4287 sec.\n",
      "Step(1300) loss: 9.4148 -- time: 4.1407 sec.\n",
      "Step(1310) loss: 7.3507 -- time: 4.3228 sec.\n",
      "Step(1320) loss: 7.5521 -- time: 4.1647 sec.\n",
      "Step(1330) loss: 7.2331 -- time: 4.2925 sec.\n",
      "Step(1340) loss: 4.8314 -- time: 4.2200 sec.\n",
      "Step(1350) loss: 7.0625 -- time: 4.3755 sec.\n",
      "Step(1360) loss: 8.6513 -- time: 4.3517 sec.\n",
      "Step(1370) loss: 6.6575 -- time: 4.1608 sec.\n",
      "Step(1380) loss: 6.1017 -- time: 4.1638 sec.\n",
      "Step(1390) loss: 9.5930 -- time: 4.3347 sec.\n",
      "Step(1400) loss: 7.3596 -- time: 4.1964 sec.\n",
      "Step(1410) loss: 8.2408 -- time: 4.2500 sec.\n",
      "Step(1420) loss: 6.9394 -- time: 4.3135 sec.\n",
      "Step(1430) loss: 8.2073 -- time: 4.3256 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 1502.4821 - val_loss: 0.0000\n",
      "time: 78.1872 sec.\n",
      "time_left: 250.1991 min\n",
      "----------------------------------------------------------------------\n",
      "Epoch 9/200\n",
      "----------------------------------------------------------------------\n",
      "Step(1440) loss: 6.2841 -- time: 3.9182 sec.\n",
      "Step(1450) loss: 7.6774 -- time: 4.2835 sec.\n",
      "Step(1460) loss: 11.2518 -- time: 4.3648 sec.\n",
      "Step(1470) loss: 8.0733 -- time: 4.1697 sec.\n",
      "Step(1480) loss: 7.5427 -- time: 4.3596 sec.\n",
      "Step(1490) loss: 9.3277 -- time: 4.2514 sec.\n",
      "Step(1500) loss: 10.5034 -- time: 4.6347 sec.\n",
      "Step(1510) loss: 6.7620 -- time: 4.3082 sec.\n",
      "Step(1520) loss: 8.8147 -- time: 4.7227 sec.\n",
      "Step(1530) loss: 8.1308 -- time: 4.2150 sec.\n",
      "Step(1540) loss: 8.6933 -- time: 4.2251 sec.\n",
      "Step(1550) loss: 7.6521 -- time: 4.3471 sec.\n",
      "Step(1560) loss: 10.4752 -- time: 4.3371 sec.\n",
      "Step(1570) loss: 8.3198 -- time: 4.2182 sec.\n",
      "Step(1580) loss: 5.2709 -- time: 4.3350 sec.\n",
      "Step(1590) loss: 9.0231 -- time: 4.2708 sec.\n",
      "Step(1600) loss: 9.0416 -- time: 4.3227 sec.\n",
      "Step(1610) loss: 9.8512 -- time: 4.4581 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 1491.9044 - val_loss: 0.0000\n",
      "time: 79.9781 sec.\n",
      "time_left: 254.5969 min\n",
      "----------------------------------------------------------------------\n",
      "Epoch 10/200\n",
      "----------------------------------------------------------------------\n",
      "Step(1620) loss: 7.1547 -- time: 4.0937 sec.\n",
      "Step(1630) loss: 9.6058 -- time: 4.3960 sec.\n",
      "Step(1640) loss: 10.0333 -- time: 4.2536 sec.\n",
      "Step(1650) loss: 7.6315 -- time: 4.3280 sec.\n",
      "Step(1660) loss: 22.3734 -- time: 4.1450 sec.\n",
      "Step(1670) loss: 9.4221 -- time: 4.2791 sec.\n",
      "Step(1680) loss: 6.8371 -- time: 4.3904 sec.\n",
      "Step(1690) loss: 7.9495 -- time: 4.3187 sec.\n",
      "Step(1700) loss: 9.0320 -- time: 4.2573 sec.\n",
      "Step(1710) loss: 8.6144 -- time: 4.1588 sec.\n",
      "Step(1720) loss: 6.4251 -- time: 4.1894 sec.\n",
      "Step(1730) loss: 6.9287 -- time: 4.4218 sec.\n",
      "Step(1740) loss: 8.3134 -- time: 4.3641 sec.\n",
      "Step(1750) loss: 4.4741 -- time: 4.5089 sec.\n",
      "Step(1760) loss: 8.9730 -- time: 4.3436 sec.\n",
      "Step(1770) loss: 5.7788 -- time: 4.4802 sec.\n",
      "Step(1780) loss: 6.5091 -- time: 4.3523 sec.\n",
      "Step(1790) loss: 9.3145 -- time: 4.4742 sec.\n",
      "----------------------------------------------------------------------\n",
      "(validation)\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 1462.0581 - val_loss: 1375.4786\n",
      "time: 115.9217 sec.\n",
      "time_left: 367.0855 min\n",
      "--saved weights--\n",
      "----------------------------------------------------------------------\n",
      "Epoch 11/200\n",
      "----------------------------------------------------------------------\n",
      "Step(1800) loss: 6.6265 -- time: 5.1031 sec.\n",
      "Step(1810) loss: 7.2837 -- time: 4.6350 sec.\n",
      "Step(1820) loss: 7.0030 -- time: 5.1338 sec.\n",
      "Step(1830) loss: 7.7156 -- time: 4.8114 sec.\n",
      "Step(1840) loss: 9.2051 -- time: 4.3659 sec.\n",
      "Step(1850) loss: 9.5638 -- time: 4.4463 sec.\n",
      "Step(1860) loss: 11.2871 -- time: 4.5576 sec.\n",
      "Step(1870) loss: 8.2383 -- time: 4.9991 sec.\n",
      "Step(1880) loss: 4.5054 -- time: 4.8962 sec.\n",
      "Step(1890) loss: 6.0269 -- time: 5.0202 sec.\n",
      "Step(1900) loss: 7.0777 -- time: 4.8929 sec.\n",
      "Step(1910) loss: 5.0389 -- time: 4.7354 sec.\n",
      "Step(1920) loss: 5.5121 -- time: 4.7643 sec.\n",
      "Step(1930) loss: 8.5068 -- time: 4.7779 sec.\n",
      "Step(1940) loss: 10.0977 -- time: 4.7782 sec.\n",
      "Step(1950) loss: 4.5341 -- time: 4.9898 sec.\n",
      "Step(1960) loss: 8.7895 -- time: 4.8024 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 1415.4420 - val_loss: 0.0000\n",
      "time: 87.5683 sec.\n",
      "time_left: 275.8400 min\n",
      "----------------------------------------------------------------------\n",
      "Epoch 12/200\n",
      "----------------------------------------------------------------------\n",
      "Step(1970) loss: 8.3619 -- time: 0.4135 sec.\n",
      "Step(1980) loss: 4.4940 -- time: 4.7281 sec.\n",
      "Step(1990) loss: 7.2398 -- time: 4.8357 sec.\n",
      "Step(2000) loss: 6.0631 -- time: 4.7541 sec.\n",
      "Step(2010) loss: 5.8478 -- time: 5.7304 sec.\n",
      "Step(2020) loss: 9.4968 -- time: 4.7473 sec.\n",
      "Step(2030) loss: 7.9834 -- time: 4.7982 sec.\n",
      "Step(2040) loss: 8.4072 -- time: 4.9009 sec.\n",
      "Step(2050) loss: 7.1529 -- time: 4.7217 sec.\n",
      "Step(2060) loss: 8.3847 -- time: 4.8133 sec.\n",
      "Step(2070) loss: 8.3818 -- time: 4.5648 sec.\n",
      "Step(2080) loss: 5.1578 -- time: 4.5943 sec.\n",
      "Step(2090) loss: 7.8899 -- time: 4.6691 sec.\n",
      "Step(2100) loss: 6.8131 -- time: 4.6136 sec.\n",
      "Step(2110) loss: 6.7903 -- time: 4.8882 sec.\n",
      "Step(2120) loss: 9.5297 -- time: 4.7265 sec.\n",
      "Step(2130) loss: 7.7518 -- time: 4.7833 sec.\n",
      "Step(2140) loss: 7.5083 -- time: 4.9690 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 1455.5379 - val_loss: 0.0000\n",
      "time: 87.7239 sec.\n",
      "time_left: 274.8682 min\n",
      "----------------------------------------------------------------------\n",
      "Epoch 13/200\n",
      "----------------------------------------------------------------------\n",
      "Step(2150) loss: 5.4173 -- time: 0.8720 sec.\n",
      "Step(2160) loss: 4.8802 -- time: 4.8639 sec.\n",
      "Step(2170) loss: 7.6569 -- time: 4.8307 sec.\n",
      "Step(2180) loss: 13.3165 -- time: 4.6302 sec.\n",
      "Step(2190) loss: 7.8963 -- time: 4.7492 sec.\n",
      "Step(2200) loss: 7.8236 -- time: 4.9905 sec.\n",
      "Step(2210) loss: 6.2843 -- time: 4.4261 sec.\n",
      "Step(2220) loss: 10.9069 -- time: 4.3786 sec.\n",
      "Step(2230) loss: 8.1691 -- time: 4.4078 sec.\n",
      "Step(2240) loss: 5.9658 -- time: 4.4094 sec.\n",
      "Step(2250) loss: 11.1541 -- time: 4.4453 sec.\n",
      "Step(2260) loss: 5.3267 -- time: 4.5322 sec.\n",
      "Step(2270) loss: 8.8707 -- time: 4.4306 sec.\n",
      "Step(2280) loss: 10.3612 -- time: 4.2904 sec.\n",
      "Step(2290) loss: 10.8175 -- time: 4.7573 sec.\n",
      "Step(2300) loss: 11.7180 -- time: 4.7830 sec.\n",
      "Step(2310) loss: 8.6355 -- time: 4.6883 sec.\n",
      "Step(2320) loss: 8.0492 -- time: 4.9851 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 1428.4977 - val_loss: 0.0000\n",
      "time: 84.6209 sec.\n",
      "time_left: 263.7353 min\n",
      "----------------------------------------------------------------------\n",
      "Epoch 14/200\n",
      "----------------------------------------------------------------------\n",
      "Step(2330) loss: 5.2724 -- time: 1.3521 sec.\n",
      "Step(2340) loss: 8.7432 -- time: 4.7165 sec.\n",
      "Step(2350) loss: 8.6047 -- time: 4.7044 sec.\n",
      "Step(2360) loss: 7.7842 -- time: 4.6946 sec.\n",
      "Step(2370) loss: 8.2490 -- time: 5.1231 sec.\n",
      "Step(2380) loss: 6.4078 -- time: 5.1705 sec.\n",
      "Step(2390) loss: 5.9454 -- time: 4.8823 sec.\n",
      "Step(2400) loss: 8.0762 -- time: 5.1258 sec.\n",
      "Step(2410) loss: 6.3274 -- time: 4.6869 sec.\n",
      "Step(2420) loss: 9.7755 -- time: 5.0351 sec.\n",
      "Step(2430) loss: 8.4044 -- time: 4.8781 sec.\n",
      "Step(2440) loss: 13.2384 -- time: 4.9129 sec.\n",
      "Step(2450) loss: 8.3981 -- time: 5.0175 sec.\n",
      "Step(2460) loss: 7.6979 -- time: 4.9576 sec.\n",
      "Step(2470) loss: 6.8056 -- time: 4.8751 sec.\n",
      "Step(2480) loss: 6.3475 -- time: 4.9060 sec.\n",
      "Step(2490) loss: 11.5086 -- time: 4.9232 sec.\n",
      "Step(2500) loss: 8.7853 -- time: 4.6920 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 1488.2539 - val_loss: 0.0000\n",
      "time: 89.2625 sec.\n",
      "time_left: 276.7136 min\n",
      "----------------------------------------------------------------------\n",
      "Epoch 15/200\n",
      "----------------------------------------------------------------------\n",
      "Step(2510) loss: 5.7337 -- time: 1.8347 sec.\n",
      "Step(2520) loss: 6.8020 -- time: 4.7856 sec.\n",
      "Step(2530) loss: 7.3885 -- time: 4.3112 sec.\n",
      "Step(2540) loss: 8.4812 -- time: 4.5050 sec.\n",
      "Step(2550) loss: 7.9047 -- time: 4.3812 sec.\n",
      "Step(2560) loss: 7.4983 -- time: 4.2803 sec.\n",
      "Step(2570) loss: 6.2527 -- time: 4.4009 sec.\n",
      "Step(2580) loss: 6.5731 -- time: 4.2779 sec.\n",
      "Step(2590) loss: 9.1754 -- time: 4.2944 sec.\n",
      "Step(2600) loss: 9.5699 -- time: 4.3928 sec.\n",
      "Step(2610) loss: 8.0022 -- time: 4.2979 sec.\n",
      "Step(2620) loss: 8.7643 -- time: 4.2937 sec.\n",
      "Step(2630) loss: 8.2388 -- time: 4.3356 sec.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:4\u001b[0m\n",
      "Cell \u001b[1;32mIn[5], line 55\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net, dataloaders_dict, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(phase\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     54\u001b[0m     outputs \u001b[39m=\u001b[39m net(images)\n\u001b[1;32m---> 55\u001b[0m     loss_l, loss_c \u001b[39m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m     56\u001b[0m     loss \u001b[39m=\u001b[39m loss_l \u001b[39m+\u001b[39m loss_c\n\u001b[0;32m     58\u001b[0m     \u001b[39mif\u001b[39;00m phase \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ShimaSef\\anaconda3\\envs\\env1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ShimaSef\\object_detection\\ssd.py:519\u001b[0m, in \u001b[0;36mMultiBoxLoss.forward\u001b[1;34m(self, predictions, targets)\u001b[0m\n\u001b[0;32m    516\u001b[0m num_dbox \u001b[39m=\u001b[39m loc_data\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m)\n\u001b[0;32m    517\u001b[0m num_classes \u001b[39m=\u001b[39m conf_data\u001b[39m.\u001b[39msize(\u001b[39m2\u001b[39m)\n\u001b[1;32m--> 519\u001b[0m conf_t_label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mLongTensor(num_batch, num_dbox)\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[0;32m    520\u001b[0m loc_t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(num_batch, num_dbox, \u001b[39m4\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    522\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_batch):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"0\"\n",
    "num_epochs = 200\n",
    "train(\n",
    "    net,\n",
    "    dataloaders_dict,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m現在のセルまたは前のセルでコードを実行中に、カーネルがクラッシュしました。エラーの原因を特定するには、セル内のコードを確認してください。詳細については、<a href='https://aka.ms/vscodeJupyterKernelCrash'>こちら</a> をクリックしてください。さらなる詳細については、Jupyter [log] (command:jupyter.viewOutput) を参照してください。"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"0\"\n",
    "# print(os.environ['CUDA_LAUNCH_BLOCKING'])\n",
    "# csvidx = os.listdir('loss_csv')\n",
    "# print(csvidx)\n",
    "# csvidx = [int(os.path.splitext(i)[0].split('_')[2]) for i in csvidx]\n",
    "# csvpath = '/Users/ShimaSef/object_detection/loss_csv/epoch_loss_{}.csv'.format(max(csvidx)+1)\n",
    "# print(csvpath)\n",
    "# weightidx = os.listdir('weights')\n",
    "# weightidx = [int(f.split('_')[2]) for f in weightidx if os.path.isdir(os.path.join('weights', f))]\n",
    "# if not os.path.exists('/Users/ShimaSef/object_detection/weights/ssd_weights_{}/'.format(max(weightidx)+1)):\n",
    "#   os.mkdir('/Users/ShimaSef/object_detection/weights/ssd_weights_{}/'.format(max(weightidx)+1))\n",
    "# max(csvidx)\n",
    "# print(csvidx)\n",
    "# weightidx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
