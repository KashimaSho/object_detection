{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4090\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# ---------- edit ---------- #\n",
    "print(torch.cuda.get_device_name(torch.device('cuda')))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "# device = torch.device('cpu')\n",
    "print(device)\n",
    "# -------------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "from voc import make_filepath_list, GetBBoxAndLabel, DataTransform, multiobject_collate_fn\n",
    "from preprocessDataset import PreprocessVOC2012\n",
    "\n",
    "# rootpath = '/home/masakibandai/object_detection/data/VOCdevkit/VOC2012/'\n",
    "rootpath = '/Users/ShimaSef/object_detection/data/VOCdevkit/VOC2012/'\n",
    "train_img_list, train_anno_list, val_img_list, val_anno_list = make_filepath_list(rootpath)\n",
    "voc_classes = [\n",
    "    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', \n",
    "    'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n",
    "]\n",
    "\n",
    "color_mean = (104, 117, 123)\n",
    "input_size = 300\n",
    "train_dataset = PreprocessVOC2012(\n",
    "    train_img_list,\n",
    "    train_anno_list,\n",
    "    phase='train',\n",
    "    transform=DataTransform(input_size, color_mean),\n",
    "    get_bbox_label=GetBBoxAndLabel(voc_classes)\n",
    ")\n",
    "\n",
    "val_dataset = PreprocessVOC2012(\n",
    "    val_img_list,\n",
    "    val_anno_list,\n",
    "    phase='val',\n",
    "    transform=DataTransform(input_size, color_mean),\n",
    "    get_bbox_label=GetBBoxAndLabel(voc_classes)\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataloader = data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=multiobject_collate_fn\n",
    ")\n",
    "\n",
    "val_dataloader = data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=multiobject_collate_fn\n",
    ")\n",
    "\n",
    "dataloaders_dict = {'train': train_dataloader, 'val': val_dataloader}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
       "===================================================================================================================\n",
       "SSD                                      [32, 3, 300, 300]         [32, 8732, 4]             --\n",
       "├─ModuleList: 1-3                        --                        --                        (recursive)\n",
       "│    └─Conv2d: 2-1                       [32, 3, 300, 300]         [32, 64, 300, 300]        1,792\n",
       "│    └─ReLU: 2-2                         [32, 64, 300, 300]        [32, 64, 300, 300]        --\n",
       "│    └─Conv2d: 2-3                       [32, 64, 300, 300]        [32, 64, 300, 300]        36,928\n",
       "│    └─ReLU: 2-4                         [32, 64, 300, 300]        [32, 64, 300, 300]        --\n",
       "│    └─MaxPool2d: 2-5                    [32, 64, 300, 300]        [32, 64, 150, 150]        --\n",
       "│    └─Conv2d: 2-6                       [32, 64, 150, 150]        [32, 128, 150, 150]       73,856\n",
       "│    └─ReLU: 2-7                         [32, 128, 150, 150]       [32, 128, 150, 150]       --\n",
       "│    └─Conv2d: 2-8                       [32, 128, 150, 150]       [32, 128, 150, 150]       147,584\n",
       "│    └─ReLU: 2-9                         [32, 128, 150, 150]       [32, 128, 150, 150]       --\n",
       "│    └─MaxPool2d: 2-10                   [32, 128, 150, 150]       [32, 128, 75, 75]         --\n",
       "│    └─Conv2d: 2-11                      [32, 128, 75, 75]         [32, 256, 75, 75]         295,168\n",
       "│    └─ReLU: 2-12                        [32, 256, 75, 75]         [32, 256, 75, 75]         --\n",
       "│    └─Conv2d: 2-13                      [32, 256, 75, 75]         [32, 256, 75, 75]         590,080\n",
       "│    └─ReLU: 2-14                        [32, 256, 75, 75]         [32, 256, 75, 75]         --\n",
       "│    └─Conv2d: 2-15                      [32, 256, 75, 75]         [32, 256, 75, 75]         590,080\n",
       "│    └─ReLU: 2-16                        [32, 256, 75, 75]         [32, 256, 75, 75]         --\n",
       "│    └─MaxPool2d: 2-17                   [32, 256, 75, 75]         [32, 256, 38, 38]         --\n",
       "│    └─Conv2d: 2-18                      [32, 256, 38, 38]         [32, 512, 38, 38]         1,180,160\n",
       "│    └─ReLU: 2-19                        [32, 512, 38, 38]         [32, 512, 38, 38]         --\n",
       "│    └─Conv2d: 2-20                      [32, 512, 38, 38]         [32, 512, 38, 38]         2,359,808\n",
       "│    └─ReLU: 2-21                        [32, 512, 38, 38]         [32, 512, 38, 38]         --\n",
       "│    └─Conv2d: 2-22                      [32, 512, 38, 38]         [32, 512, 38, 38]         2,359,808\n",
       "│    └─ReLU: 2-23                        [32, 512, 38, 38]         [32, 512, 38, 38]         --\n",
       "├─L2Norm: 1-2                            [32, 512, 38, 38]         [32, 512, 38, 38]         512\n",
       "├─ModuleList: 1-3                        --                        --                        (recursive)\n",
       "│    └─MaxPool2d: 2-24                   [32, 512, 38, 38]         [32, 512, 19, 19]         --\n",
       "│    └─Conv2d: 2-25                      [32, 512, 19, 19]         [32, 512, 19, 19]         2,359,808\n",
       "│    └─ReLU: 2-26                        [32, 512, 19, 19]         [32, 512, 19, 19]         --\n",
       "│    └─Conv2d: 2-27                      [32, 512, 19, 19]         [32, 512, 19, 19]         2,359,808\n",
       "│    └─ReLU: 2-28                        [32, 512, 19, 19]         [32, 512, 19, 19]         --\n",
       "│    └─Conv2d: 2-29                      [32, 512, 19, 19]         [32, 512, 19, 19]         2,359,808\n",
       "│    └─ReLU: 2-30                        [32, 512, 19, 19]         [32, 512, 19, 19]         --\n",
       "│    └─MaxPool2d: 2-31                   [32, 512, 19, 19]         [32, 512, 19, 19]         --\n",
       "│    └─Conv2d: 2-32                      [32, 512, 19, 19]         [32, 1024, 19, 19]        4,719,616\n",
       "│    └─ReLU: 2-33                        [32, 1024, 19, 19]        [32, 1024, 19, 19]        --\n",
       "│    └─Conv2d: 2-34                      [32, 1024, 19, 19]        [32, 1024, 19, 19]        1,049,600\n",
       "│    └─ReLU: 2-35                        [32, 1024, 19, 19]        [32, 1024, 19, 19]        --\n",
       "├─ModuleList: 1-4                        --                        --                        --\n",
       "│    └─Conv2d: 2-36                      [32, 1024, 19, 19]        [32, 256, 19, 19]         262,400\n",
       "│    └─Conv2d: 2-37                      [32, 256, 19, 19]         [32, 512, 10, 10]         1,180,160\n",
       "│    └─Conv2d: 2-38                      [32, 512, 10, 10]         [32, 128, 10, 10]         65,664\n",
       "│    └─Conv2d: 2-39                      [32, 128, 10, 10]         [32, 256, 5, 5]           295,168\n",
       "│    └─Conv2d: 2-40                      [32, 256, 5, 5]           [32, 128, 5, 5]           32,896\n",
       "│    └─Conv2d: 2-41                      [32, 128, 5, 5]           [32, 256, 3, 3]           295,168\n",
       "│    └─Conv2d: 2-42                      [32, 256, 3, 3]           [32, 128, 3, 3]           32,896\n",
       "│    └─Conv2d: 2-43                      [32, 128, 3, 3]           [32, 256, 1, 1]           295,168\n",
       "├─ModuleList: 1-15                       --                        --                        (recursive)\n",
       "│    └─Conv2d: 2-44                      [32, 512, 38, 38]         [32, 16, 38, 38]          73,744\n",
       "├─ModuleList: 1-16                       --                        --                        (recursive)\n",
       "│    └─Conv2d: 2-45                      [32, 512, 38, 38]         [32, 84, 38, 38]          387,156\n",
       "├─ModuleList: 1-15                       --                        --                        (recursive)\n",
       "│    └─Conv2d: 2-46                      [32, 1024, 19, 19]        [32, 24, 19, 19]          221,208\n",
       "├─ModuleList: 1-16                       --                        --                        (recursive)\n",
       "│    └─Conv2d: 2-47                      [32, 1024, 19, 19]        [32, 126, 19, 19]         1,161,342\n",
       "├─ModuleList: 1-15                       --                        --                        (recursive)\n",
       "│    └─Conv2d: 2-48                      [32, 512, 10, 10]         [32, 24, 10, 10]          110,616\n",
       "├─ModuleList: 1-16                       --                        --                        (recursive)\n",
       "│    └─Conv2d: 2-49                      [32, 512, 10, 10]         [32, 126, 10, 10]         580,734\n",
       "├─ModuleList: 1-15                       --                        --                        (recursive)\n",
       "│    └─Conv2d: 2-50                      [32, 256, 5, 5]           [32, 24, 5, 5]            55,320\n",
       "├─ModuleList: 1-16                       --                        --                        (recursive)\n",
       "│    └─Conv2d: 2-51                      [32, 256, 5, 5]           [32, 126, 5, 5]           290,430\n",
       "├─ModuleList: 1-15                       --                        --                        (recursive)\n",
       "│    └─Conv2d: 2-52                      [32, 256, 3, 3]           [32, 16, 3, 3]            36,880\n",
       "├─ModuleList: 1-16                       --                        --                        (recursive)\n",
       "│    └─Conv2d: 2-53                      [32, 256, 3, 3]           [32, 84, 3, 3]            193,620\n",
       "├─ModuleList: 1-15                       --                        --                        (recursive)\n",
       "│    └─Conv2d: 2-54                      [32, 256, 1, 1]           [32, 16, 1, 1]            36,880\n",
       "├─ModuleList: 1-16                       --                        --                        (recursive)\n",
       "│    └─Conv2d: 2-55                      [32, 256, 1, 1]           [32, 84, 1, 1]            193,620\n",
       "===================================================================================================================\n",
       "Total params: 26,285,486\n",
       "Trainable params: 26,285,486\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (T): 1.00\n",
       "===================================================================================================================\n",
       "Input size (MB): 34.56\n",
       "Forward/backward pass size (MB): 6717.23\n",
       "Params size (MB): 105.14\n",
       "Estimated Total Size (MB): 6856.93\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from ssd import SSD\n",
    "from torchinfo import summary\n",
    "\n",
    "ssd_cfg = {\n",
    "    'classes_num': 21,\n",
    "    'input_size': 300,\n",
    "    'dbox_num': [4, 6, 6, 6, 4, 4],\n",
    "    'feature_maps': [38, 19, 10, 5, 3, 1],\n",
    "    'steps': [8, 16, 32, 64, 100, 300],\n",
    "    'min_sizes': [30, 60, 111, 162, 213, 264],\n",
    "    'max_sizes': [60, 111, 162, 213, 264, 315],\n",
    "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "}\n",
    "\n",
    "net = SSD(phase='train', cfg=ssd_cfg)\n",
    "# weightpath = '/home/masakibandai/object_detection/weights/vgg16_reducedfc.pth'\n",
    "weightpath = '/Users/ShimaSef/object_detection/weights/vgg16_reducedfc.pth'\n",
    "vgg_weights = torch.load(weightpath)\n",
    "net.vgg.load_state_dict(vgg_weights)\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "net.extras.apply(weights_init)\n",
    "net.loc.apply(weights_init)\n",
    "net.conf.apply(weights_init)\n",
    "\n",
    "summary(\n",
    "    net,\n",
    "    input_size=(batch_size, 3, 300, 300),\n",
    "    col_names=['input_size', 'output_size', 'num_params']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from ssd import MultiBoxLoss\n",
    "\n",
    "criterion = MultiBoxLoss(\n",
    "    jaccard_thresh=0.5,\n",
    "    neg_pos=3,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "    net.parameters(),\n",
    "    lr=1e-3,\n",
    "    momentum=0.9,\n",
    "    weight_decay=5e-4\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def train(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "    '''\n",
    "    Parameter:\n",
    "        net(object): SSD model\n",
    "        dataloaders_dict(dict of object): dataloader\n",
    "        criterion(object): loss function\n",
    "        optimizer(object): optimizer\n",
    "        num_epochs(object): num learning\n",
    "    '''\n",
    "    print(device)\n",
    "    # ---------edit--------- #\n",
    "    net.to(device)\n",
    "    # net.cuda()\n",
    "    # ---------------------- #\n",
    "    print('Start training with {}'.format(torch.cuda.get_device_name()))\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    iteration = 1\n",
    "    epoch_train_loss = 0.0\n",
    "    epoch_val_loss = 0.0\n",
    "    logs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        t_epoch_start = time.time()\n",
    "        t_iter_start = time.time()\n",
    "\n",
    "        print('----------------------------------------------------------------------')\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('----------------------------------------------------------------------')\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()\n",
    "            else:\n",
    "                if ((epoch+1) % 10 == 0):\n",
    "                    net.eval()\n",
    "                    print('----------------------------------------------------------------------')\n",
    "                    print('(validation)')\n",
    "                else:\n",
    "                    continue\n",
    "        \n",
    "            for images, targets in dataloaders_dict[phase]:\n",
    "                images = images.to(device)\n",
    "                targets = [ann.to(device) for ann in targets]\n",
    "                # ---------edit---------- #\n",
    "                # images.cuda()\n",
    "                # targets.cuda()\n",
    "                # ----------------------- #\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase=='train'):\n",
    "                    outputs = net(images)\n",
    "                    loss_l, loss_c = criterion(outputs, targets)\n",
    "                    loss = loss_l + loss_c\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        nn.utils.clip_grad_value_(net.parameters(), clip_value=2.0)\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        if (iteration % 10 == 0):\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            print('Step({}) loss: {:.4f} -- time: {:.4f} sec.'.format(iteration, loss.item(), duration))\n",
    "                            t_iter_start = time.time()\n",
    "                        \n",
    "                        epoch_train_loss += loss.item()\n",
    "                        iteration += 1\n",
    "\n",
    "                    else:\n",
    "                        epoch_val_loss += loss.item()\n",
    "        t_epoch_finish = time.time()\n",
    "        print('----------------------------------------------------------------------')\n",
    "\n",
    "        print('train_loss: {:.4f} - val_loss: {:.4f}'.format(epoch_train_loss, epoch_val_loss))\n",
    "        print('time: {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
    "        print('time_left: {:.4f} min'.format((t_epoch_finish-t_epoch_start)*(num_epochs-epoch-1)/60))\n",
    "\n",
    "        t_epoch_start = time.time()\n",
    "        log_epoch = {\n",
    "            'epoch': epoch+1,\n",
    "            'train_loss': epoch_train_loss,\n",
    "            'val_loss': epoch_val_loss\n",
    "        }\n",
    "\n",
    "        logs.append(log_epoch)\n",
    "        df = pd.DataFrame(logs)\n",
    "        # csvpath = '/home/masakibandai/object_detection/epoch_loss.csv'\n",
    "        csvpath = '/Users/ShimaSef/object_detection/epoch_loss.csv'\n",
    "        df.to_csv(csvpath)\n",
    "        epoch_train_loss = 0.0\n",
    "        epoch_val_loss = 0.0\n",
    "        # statedictpath = '/home/masakibandai/object_detection/weights/ssd_weights'\n",
    "        statedictpath = '/Users/ShimaSef/object_detection/weights/ssd_weights'\n",
    "\n",
    "        if ((epoch+1) % 10 == 0):\n",
    "            torch.save(\n",
    "                net.state_dict(),\n",
    "                 statedictpath + str(epoch+1) + '.pth'\n",
    "            )\n",
    "            print('--saved weights--')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Start training with NVIDIA GeForce RTX 4090\n",
      "----------------------------------------------------------------------\n",
      "Epoch 1/50\n",
      "----------------------------------------------------------------------\n",
      "Step(10) loss: 21.3681 -- time: 11.0122 sec.\n",
      "Step(20) loss: 18.4272 -- time: 4.5779 sec.\n",
      "Step(30) loss: 17.0787 -- time: 4.6056 sec.\n",
      "Step(40) loss: 20.1232 -- time: 4.6048 sec.\n",
      "Step(50) loss: 17.6484 -- time: 4.5731 sec.\n",
      "Step(60) loss: 16.5138 -- time: 4.4498 sec.\n",
      "Step(70) loss: 18.5425 -- time: 4.6401 sec.\n",
      "Step(80) loss: 14.3611 -- time: 4.5248 sec.\n",
      "Step(90) loss: 30.5919 -- time: 4.4854 sec.\n",
      "Step(100) loss: 18.1957 -- time: 4.6542 sec.\n",
      "Step(110) loss: 18.4525 -- time: 4.6558 sec.\n",
      "Step(120) loss: 19.1690 -- time: 4.6590 sec.\n",
      "Step(130) loss: 17.9382 -- time: 4.5534 sec.\n",
      "Step(140) loss: 15.2047 -- time: 4.4710 sec.\n",
      "Step(150) loss: 30.3332 -- time: 4.5779 sec.\n",
      "Step(160) loss: 19.0474 -- time: 4.4399 sec.\n",
      "Step(170) loss: 8.8973 -- time: 4.5252 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 3015.3571 - val_loss: 0.0000\n",
      "time: 96.3902 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 2/50\n",
      "----------------------------------------------------------------------\n",
      "Step(180) loss: 8.5242 -- time: 0.4517 sec.\n",
      "Step(190) loss: 6.7671 -- time: 4.6119 sec.\n",
      "Step(200) loss: 15.2164 -- time: 4.5695 sec.\n",
      "Step(210) loss: 9.1901 -- time: 4.5907 sec.\n",
      "Step(220) loss: 9.4776 -- time: 4.4468 sec.\n",
      "Step(230) loss: 17.9766 -- time: 4.6412 sec.\n",
      "Step(240) loss: 18.1239 -- time: 4.5452 sec.\n",
      "Step(250) loss: 11.3730 -- time: 4.4573 sec.\n",
      "Step(260) loss: 19.7652 -- time: 4.7158 sec.\n",
      "Step(270) loss: 9.2052 -- time: 4.6277 sec.\n",
      "Step(280) loss: 8.7724 -- time: 4.6731 sec.\n",
      "Step(290) loss: 60.0103 -- time: 4.6000 sec.\n",
      "Step(300) loss: 6.7495 -- time: 4.4332 sec.\n",
      "Step(310) loss: 15.4450 -- time: 4.6628 sec.\n",
      "Step(320) loss: 4.7451 -- time: 4.5560 sec.\n",
      "Step(330) loss: 9.5786 -- time: 4.5887 sec.\n",
      "Step(340) loss: 65.2082 -- time: 4.5343 sec.\n",
      "Step(350) loss: 9.9100 -- time: 4.5581 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 2507.5683 - val_loss: 0.0000\n",
      "time: 83.5908 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 3/50\n",
      "----------------------------------------------------------------------\n",
      "Step(360) loss: 13.7186 -- time: 0.7956 sec.\n",
      "Step(370) loss: 10.4306 -- time: 4.5933 sec.\n",
      "Step(380) loss: 5.9091 -- time: 4.5980 sec.\n",
      "Step(390) loss: 7.1106 -- time: 4.4301 sec.\n",
      "Step(400) loss: 7.9348 -- time: 4.4746 sec.\n",
      "Step(410) loss: 9.0561 -- time: 4.4638 sec.\n",
      "Step(420) loss: 4.1162 -- time: 4.5712 sec.\n",
      "Step(430) loss: 13.5851 -- time: 4.3598 sec.\n",
      "Step(440) loss: 5.7061 -- time: 4.4828 sec.\n",
      "Step(450) loss: 8.9559 -- time: 4.4710 sec.\n",
      "Step(460) loss: 9.4272 -- time: 4.6510 sec.\n",
      "Step(470) loss: 11.0882 -- time: 4.4200 sec.\n",
      "Step(480) loss: 43.8674 -- time: 4.5159 sec.\n",
      "Step(490) loss: 8.4177 -- time: 4.4963 sec.\n",
      "Step(500) loss: 5.5408 -- time: 4.6343 sec.\n",
      "Step(510) loss: 11.2951 -- time: 4.5237 sec.\n",
      "Step(520) loss: 5.7932 -- time: 4.5947 sec.\n",
      "Step(530) loss: 7.7785 -- time: 4.5633 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 2346.2111 - val_loss: 0.0000\n",
      "time: 82.6090 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 4/50\n",
      "----------------------------------------------------------------------\n",
      "Step(540) loss: 10.4036 -- time: 1.2565 sec.\n",
      "Step(550) loss: 7.6950 -- time: 4.5452 sec.\n",
      "Step(560) loss: 7.6928 -- time: 4.7144 sec.\n",
      "Step(570) loss: 8.4556 -- time: 4.6281 sec.\n",
      "Step(580) loss: 9.3013 -- time: 4.5388 sec.\n",
      "Step(590) loss: 8.3421 -- time: 4.6870 sec.\n",
      "Step(600) loss: 7.9079 -- time: 4.5476 sec.\n",
      "Step(610) loss: 8.6967 -- time: 4.5303 sec.\n",
      "Step(620) loss: 15.1028 -- time: 4.5418 sec.\n",
      "Step(630) loss: 5.1644 -- time: 4.4917 sec.\n",
      "Step(640) loss: 71.3016 -- time: 4.4979 sec.\n",
      "Step(650) loss: 33.5908 -- time: 4.5704 sec.\n",
      "Step(660) loss: 7.9416 -- time: 4.5904 sec.\n",
      "Step(670) loss: 25.4171 -- time: 4.6406 sec.\n",
      "Step(680) loss: 8.3232 -- time: 4.4360 sec.\n",
      "Step(690) loss: 5.6312 -- time: 4.5286 sec.\n",
      "Step(700) loss: 9.2293 -- time: 4.2656 sec.\n",
      "Step(710) loss: 19.8096 -- time: 4.4642 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 2931.6434 - val_loss: 0.0000\n",
      "time: 82.9818 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 5/50\n",
      "----------------------------------------------------------------------\n",
      "Step(720) loss: 7.5625 -- time: 1.7589 sec.\n",
      "Step(730) loss: 4.7376 -- time: 4.5041 sec.\n",
      "Step(740) loss: 6.3709 -- time: 4.5475 sec.\n",
      "Step(750) loss: 8.2554 -- time: 4.5196 sec.\n",
      "Step(760) loss: 9.2688 -- time: 4.6025 sec.\n",
      "Step(770) loss: 9.7285 -- time: 4.4596 sec.\n",
      "Step(780) loss: 4.8164 -- time: 4.4852 sec.\n",
      "Step(790) loss: 7.5246 -- time: 4.4716 sec.\n",
      "Step(800) loss: 163.8066 -- time: 4.3740 sec.\n",
      "Step(810) loss: 16.1826 -- time: 4.5643 sec.\n",
      "Step(820) loss: 78.2021 -- time: 4.5191 sec.\n",
      "Step(830) loss: 18.8495 -- time: 4.4212 sec.\n",
      "Step(840) loss: 10.9130 -- time: 4.4551 sec.\n",
      "Step(850) loss: 10.2720 -- time: 4.6007 sec.\n",
      "Step(860) loss: 8.5362 -- time: 4.4129 sec.\n",
      "Step(870) loss: 10.6298 -- time: 4.3235 sec.\n",
      "Step(880) loss: 10.4424 -- time: 4.5258 sec.\n",
      "Step(890) loss: 10.6542 -- time: 4.6960 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 3824.5212 - val_loss: 0.0000\n",
      "time: 82.3889 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 6/50\n",
      "----------------------------------------------------------------------\n",
      "Step(900) loss: 7.6457 -- time: 2.2233 sec.\n",
      "Step(910) loss: 16.9693 -- time: 4.5997 sec.\n",
      "Step(920) loss: 6.2937 -- time: 4.6616 sec.\n",
      "Step(930) loss: 33.1307 -- time: 4.5822 sec.\n",
      "Step(940) loss: 8.0202 -- time: 4.6491 sec.\n",
      "Step(950) loss: 6.8692 -- time: 4.5103 sec.\n",
      "Step(960) loss: 6.6435 -- time: 4.6478 sec.\n",
      "Step(970) loss: 9.5044 -- time: 4.6125 sec.\n",
      "Step(980) loss: 29.1988 -- time: 4.6328 sec.\n",
      "Step(990) loss: 8.2027 -- time: 4.6231 sec.\n",
      "Step(1000) loss: 5.7141 -- time: 4.5923 sec.\n",
      "Step(1010) loss: 7.0117 -- time: 4.6061 sec.\n",
      "Step(1020) loss: 30.2331 -- time: 4.6510 sec.\n",
      "Step(1030) loss: 66.8089 -- time: 4.4507 sec.\n",
      "Step(1040) loss: 42.6355 -- time: 4.5511 sec.\n",
      "Step(1050) loss: 8.8228 -- time: 4.6161 sec.\n",
      "Step(1060) loss: 8.3351 -- time: 4.5787 sec.\n",
      "Step(1070) loss: 11.7322 -- time: 4.4569 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 2835.5573 - val_loss: 0.0000\n",
      "time: 83.8174 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 7/50\n",
      "----------------------------------------------------------------------\n",
      "Step(1080) loss: 6.0751 -- time: 2.6797 sec.\n",
      "Step(1090) loss: 7.4985 -- time: 4.5379 sec.\n",
      "Step(1100) loss: 16.8100 -- time: 4.5528 sec.\n",
      "Step(1110) loss: 7.0101 -- time: 4.5747 sec.\n",
      "Step(1120) loss: 7.4223 -- time: 4.6582 sec.\n",
      "Step(1130) loss: 18.2498 -- time: 4.6769 sec.\n",
      "Step(1140) loss: 8.8307 -- time: 4.5893 sec.\n",
      "Step(1150) loss: 6.5396 -- time: 4.4351 sec.\n",
      "Step(1160) loss: 9.7377 -- time: 4.7070 sec.\n",
      "Step(1170) loss: 7.3359 -- time: 4.6349 sec.\n",
      "Step(1180) loss: 9.3098 -- time: 4.6504 sec.\n",
      "Step(1190) loss: 9.1604 -- time: 4.5361 sec.\n",
      "Step(1200) loss: 8.4830 -- time: 4.6079 sec.\n",
      "Step(1210) loss: 7.8222 -- time: 4.4651 sec.\n",
      "Step(1220) loss: 6.7153 -- time: 4.5955 sec.\n",
      "Step(1230) loss: 6.0469 -- time: 4.3836 sec.\n",
      "Step(1240) loss: 4.5246 -- time: 4.6352 sec.\n",
      "Step(1250) loss: 16.8010 -- time: 4.5486 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 2892.2064 - val_loss: 0.0000\n",
      "time: 83.5858 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 8/50\n",
      "----------------------------------------------------------------------\n",
      "Step(1260) loss: 7.4875 -- time: 3.0472 sec.\n",
      "Step(1270) loss: 7.1211 -- time: 4.6474 sec.\n",
      "Step(1280) loss: 7.6701 -- time: 4.4887 sec.\n",
      "Step(1290) loss: 4.5396 -- time: 4.5277 sec.\n",
      "Step(1300) loss: 88.9819 -- time: 4.5119 sec.\n",
      "Step(1310) loss: 37.0704 -- time: 4.4506 sec.\n",
      "Step(1320) loss: 9.8022 -- time: 4.5780 sec.\n",
      "Step(1330) loss: 6.5105 -- time: 4.4841 sec.\n",
      "Step(1340) loss: 9.4284 -- time: 4.4986 sec.\n",
      "Step(1350) loss: 34.9730 -- time: 4.4372 sec.\n",
      "Step(1360) loss: 19.7606 -- time: 4.5705 sec.\n",
      "Step(1370) loss: 7.8027 -- time: 4.4566 sec.\n",
      "Step(1380) loss: 11.8310 -- time: 4.4403 sec.\n",
      "Step(1390) loss: 34.5879 -- time: 4.4847 sec.\n",
      "Step(1400) loss: 8.4810 -- time: 4.4414 sec.\n",
      "Step(1410) loss: 19.1529 -- time: 4.5376 sec.\n",
      "Step(1420) loss: 8.7346 -- time: 4.6752 sec.\n",
      "Step(1430) loss: 54.1353 -- time: 4.4794 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 2767.7443 - val_loss: 0.0000\n",
      "time: 82.4254 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 9/50\n",
      "----------------------------------------------------------------------\n",
      "Step(1440) loss: 11.5168 -- time: 3.5404 sec.\n",
      "Step(1450) loss: 6.8019 -- time: 4.5504 sec.\n",
      "Step(1460) loss: 6.2281 -- time: 4.5551 sec.\n",
      "Step(1470) loss: 123.7545 -- time: 4.5575 sec.\n",
      "Step(1480) loss: 6.7275 -- time: 4.5246 sec.\n",
      "Step(1490) loss: 7.6657 -- time: 4.5504 sec.\n",
      "Step(1500) loss: 5.8432 -- time: 4.6626 sec.\n",
      "Step(1510) loss: 23.2689 -- time: 4.4494 sec.\n",
      "Step(1520) loss: 25.4201 -- time: 4.6085 sec.\n",
      "Step(1530) loss: 18.9908 -- time: 4.5932 sec.\n",
      "Step(1540) loss: 13.9502 -- time: 4.5523 sec.\n",
      "Step(1550) loss: 8.1790 -- time: 4.7504 sec.\n",
      "Step(1560) loss: 41.9349 -- time: 4.4715 sec.\n",
      "Step(1570) loss: 51.7535 -- time: 4.4719 sec.\n",
      "Step(1580) loss: 7.0846 -- time: 4.4331 sec.\n",
      "Step(1590) loss: 8.5593 -- time: 4.5338 sec.\n",
      "Step(1600) loss: 69.4388 -- time: 4.4645 sec.\n",
      "Step(1610) loss: 7.5229 -- time: 4.7722 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 2891.5091 - val_loss: 0.0000\n",
      "time: 83.2508 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 10/50\n",
      "----------------------------------------------------------------------\n",
      "Step(1620) loss: 5.2630 -- time: 4.1280 sec.\n",
      "Step(1630) loss: 6.5066 -- time: 4.6993 sec.\n",
      "Step(1640) loss: 6.9606 -- time: 4.5008 sec.\n",
      "Step(1650) loss: 19.8714 -- time: 4.5088 sec.\n",
      "Step(1660) loss: 20.8230 -- time: 4.4310 sec.\n",
      "Step(1670) loss: 17.7620 -- time: 4.4770 sec.\n",
      "Step(1680) loss: 9.2642 -- time: 4.6148 sec.\n",
      "Step(1690) loss: 5.1951 -- time: 4.5508 sec.\n",
      "Step(1700) loss: 7.5683 -- time: 4.6256 sec.\n",
      "Step(1710) loss: 7.8825 -- time: 4.5196 sec.\n",
      "Step(1720) loss: 9.6905 -- time: 4.4868 sec.\n",
      "Step(1730) loss: 6.7600 -- time: 4.3695 sec.\n",
      "Step(1740) loss: 5.7435 -- time: 4.6131 sec.\n",
      "Step(1750) loss: 7.7212 -- time: 4.4721 sec.\n",
      "Step(1760) loss: 8.5309 -- time: 4.5272 sec.\n",
      "Step(1770) loss: 7.6114 -- time: 4.5352 sec.\n",
      "Step(1780) loss: 25.7448 -- time: 4.4274 sec.\n",
      "Step(1790) loss: 30.8543 -- time: 4.5793 sec.\n",
      "----------------------------------------------------------------------\n",
      "(validation)\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 2734.4621 - val_loss: 2581.9840\n",
      "time: 147.9242 sec.\n",
      "--saved weights--\n",
      "----------------------------------------------------------------------\n",
      "Epoch 11/50\n",
      "----------------------------------------------------------------------\n",
      "Step(1800) loss: 6.6442 -- time: 4.5313 sec.\n",
      "Step(1810) loss: 46.8408 -- time: 4.2395 sec.\n",
      "Step(1820) loss: 6.7876 -- time: 4.2129 sec.\n",
      "Step(1830) loss: 7.3982 -- time: 4.3382 sec.\n",
      "Step(1840) loss: 6.7992 -- time: 4.2524 sec.\n",
      "Step(1850) loss: 25.4616 -- time: 4.3141 sec.\n",
      "Step(1860) loss: 11.8289 -- time: 4.4114 sec.\n",
      "Step(1870) loss: 7.9959 -- time: 4.2687 sec.\n",
      "Step(1880) loss: 65.2633 -- time: 4.3477 sec.\n",
      "Step(1890) loss: 7.9561 -- time: 4.2338 sec.\n",
      "Step(1900) loss: 7.1056 -- time: 4.3305 sec.\n",
      "Step(1910) loss: 7.2294 -- time: 4.3017 sec.\n",
      "Step(1920) loss: 11.1943 -- time: 4.3518 sec.\n",
      "Step(1930) loss: 7.1489 -- time: 4.3086 sec.\n",
      "Step(1940) loss: 6.3398 -- time: 4.3906 sec.\n",
      "Step(1950) loss: 6.8788 -- time: 4.3826 sec.\n",
      "Step(1960) loss: 8.1415 -- time: 4.2411 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 2515.7550 - val_loss: 0.0000\n",
      "time: 79.0627 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 12/50\n",
      "----------------------------------------------------------------------\n",
      "Step(1970) loss: 6.0208 -- time: 0.3790 sec.\n",
      "Step(1980) loss: 7.2203 -- time: 4.3066 sec.\n",
      "Step(1990) loss: 8.0010 -- time: 4.1983 sec.\n",
      "Step(2000) loss: 15.3843 -- time: 4.3050 sec.\n",
      "Step(2010) loss: 11.0241 -- time: 4.3222 sec.\n",
      "Step(2020) loss: 7.1232 -- time: 4.3235 sec.\n",
      "Step(2030) loss: 8.5489 -- time: 4.3477 sec.\n",
      "Step(2040) loss: 5.2743 -- time: 4.4888 sec.\n",
      "Step(2050) loss: 9.1447 -- time: 4.3801 sec.\n",
      "Step(2060) loss: 6.5302 -- time: 4.3452 sec.\n",
      "Step(2070) loss: 18.1689 -- time: 4.1857 sec.\n",
      "Step(2080) loss: 13.4212 -- time: 4.3409 sec.\n",
      "Step(2090) loss: 5.9202 -- time: 4.3103 sec.\n",
      "Step(2100) loss: 9.3477 -- time: 4.4190 sec.\n",
      "Step(2110) loss: 8.9347 -- time: 4.3040 sec.\n",
      "Step(2120) loss: 6.2390 -- time: 4.3711 sec.\n",
      "Step(2130) loss: 9.1449 -- time: 4.2471 sec.\n",
      "Step(2140) loss: 8.5743 -- time: 4.3715 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 2900.4558 - val_loss: 0.0000\n",
      "time: 79.2155 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 13/50\n",
      "----------------------------------------------------------------------\n",
      "Step(2150) loss: 20.2010 -- time: 0.7783 sec.\n",
      "Step(2160) loss: 11.0850 -- time: 4.4513 sec.\n",
      "Step(2170) loss: 8.6288 -- time: 4.3631 sec.\n",
      "Step(2180) loss: 8.8692 -- time: 4.2639 sec.\n",
      "Step(2190) loss: 8.4606 -- time: 4.3169 sec.\n",
      "Step(2200) loss: 6.5272 -- time: 4.4012 sec.\n",
      "Step(2210) loss: 30.5277 -- time: 4.4371 sec.\n",
      "Step(2220) loss: 6.9697 -- time: 4.4691 sec.\n",
      "Step(2230) loss: 22.5971 -- time: 4.4561 sec.\n",
      "Step(2240) loss: 7.3180 -- time: 4.4267 sec.\n",
      "Step(2250) loss: 5.9537 -- time: 4.3278 sec.\n",
      "Step(2260) loss: 8.3663 -- time: 4.3895 sec.\n",
      "Step(2270) loss: 7.5173 -- time: 4.3565 sec.\n",
      "Step(2280) loss: 9.1235 -- time: 4.3133 sec.\n",
      "Step(2290) loss: 7.9896 -- time: 4.2950 sec.\n",
      "Step(2300) loss: 6.9278 -- time: 4.4264 sec.\n",
      "Step(2310) loss: 8.0293 -- time: 4.3818 sec.\n",
      "Step(2320) loss: 54.9283 -- time: 4.4464 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 2756.1544 - val_loss: 0.0000\n",
      "time: 80.1966 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 14/50\n",
      "----------------------------------------------------------------------\n",
      "Step(2330) loss: 6.4422 -- time: 1.2474 sec.\n",
      "Step(2340) loss: 13.4600 -- time: 4.3312 sec.\n",
      "Step(2350) loss: 97.0887 -- time: 4.1992 sec.\n",
      "Step(2360) loss: 8.6758 -- time: 4.1929 sec.\n",
      "Step(2370) loss: 102.2684 -- time: 4.3610 sec.\n",
      "Step(2380) loss: 4.0029 -- time: 4.4765 sec.\n",
      "Step(2390) loss: 9.0946 -- time: 4.3564 sec.\n",
      "Step(2400) loss: 8.6117 -- time: 4.3825 sec.\n",
      "Step(2410) loss: 6.6739 -- time: 4.3766 sec.\n",
      "Step(2420) loss: 3.8579 -- time: 4.3914 sec.\n",
      "Step(2430) loss: 13.3363 -- time: 4.3429 sec.\n",
      "Step(2440) loss: 7.3169 -- time: 4.3549 sec.\n",
      "Step(2450) loss: 8.0171 -- time: 4.5358 sec.\n",
      "Step(2460) loss: 8.4346 -- time: 4.3007 sec.\n",
      "Step(2470) loss: 8.3560 -- time: 4.3524 sec.\n",
      "Step(2480) loss: 29.5475 -- time: 4.3433 sec.\n",
      "Step(2490) loss: 12.4000 -- time: 4.3862 sec.\n",
      "Step(2500) loss: 8.7278 -- time: 4.6106 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 3100.9196 - val_loss: 0.0000\n",
      "time: 79.9388 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 15/50\n",
      "----------------------------------------------------------------------\n",
      "Step(2510) loss: 6.1169 -- time: 1.6199 sec.\n",
      "Step(2520) loss: 5.6632 -- time: 4.3828 sec.\n",
      "Step(2530) loss: 7.3748 -- time: 4.4280 sec.\n",
      "Step(2540) loss: 7.5165 -- time: 4.4386 sec.\n",
      "Step(2550) loss: 8.7205 -- time: 4.2094 sec.\n",
      "Step(2560) loss: 38.1998 -- time: 4.3801 sec.\n",
      "Step(2570) loss: 8.0292 -- time: 4.3327 sec.\n",
      "Step(2580) loss: 8.0534 -- time: 4.3388 sec.\n",
      "Step(2590) loss: 8.1268 -- time: 4.4675 sec.\n",
      "Step(2600) loss: 23.9538 -- time: 4.3616 sec.\n",
      "Step(2610) loss: 64.6394 -- time: 4.4860 sec.\n",
      "Step(2620) loss: 8.1443 -- time: 4.1835 sec.\n",
      "Step(2630) loss: 24.3706 -- time: 4.4738 sec.\n",
      "Step(2640) loss: 8.6333 -- time: 4.3081 sec.\n",
      "Step(2650) loss: 11.7529 -- time: 4.3863 sec.\n",
      "Step(2660) loss: 8.4347 -- time: 4.2556 sec.\n",
      "Step(2670) loss: 4.0234 -- time: 4.2835 sec.\n",
      "Step(2680) loss: 33.3623 -- time: 4.3149 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 3019.2623 - val_loss: 0.0000\n",
      "time: 79.7318 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 16/50\n",
      "----------------------------------------------------------------------\n",
      "Step(2690) loss: 10.1954 -- time: 2.1912 sec.\n",
      "Step(2700) loss: 18.6511 -- time: 4.3217 sec.\n",
      "Step(2710) loss: 8.4440 -- time: 4.4055 sec.\n",
      "Step(2720) loss: 87.9383 -- time: 4.3337 sec.\n",
      "Step(2730) loss: 5.0217 -- time: 4.3306 sec.\n",
      "Step(2740) loss: 31.9471 -- time: 4.2176 sec.\n",
      "Step(2750) loss: 8.2985 -- time: 4.1782 sec.\n",
      "Step(2760) loss: 13.2042 -- time: 4.2093 sec.\n",
      "Step(2770) loss: 8.6535 -- time: 4.2521 sec.\n",
      "Step(2780) loss: 58.9034 -- time: 4.3463 sec.\n",
      "Step(2790) loss: 61.3781 -- time: 4.2387 sec.\n",
      "Step(2800) loss: 7.8730 -- time: 4.2569 sec.\n",
      "Step(2810) loss: 4.2710 -- time: 4.3982 sec.\n",
      "Step(2820) loss: 7.6648 -- time: 4.1785 sec.\n",
      "Step(2830) loss: 75.7991 -- time: 4.3471 sec.\n",
      "Step(2840) loss: 89.7214 -- time: 4.2624 sec.\n",
      "Step(2850) loss: 5.3208 -- time: 4.4736 sec.\n",
      "Step(2860) loss: 5.9054 -- time: 4.2907 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 3069.4735 - val_loss: 0.0000\n",
      "time: 78.7476 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 17/50\n",
      "----------------------------------------------------------------------\n",
      "Step(2870) loss: 6.6284 -- time: 2.5062 sec.\n",
      "Step(2880) loss: 12.8765 -- time: 4.3293 sec.\n",
      "Step(2890) loss: 56.3628 -- time: 4.3548 sec.\n",
      "Step(2900) loss: 26.5024 -- time: 4.3370 sec.\n",
      "Step(2910) loss: 7.6563 -- time: 4.2363 sec.\n",
      "Step(2920) loss: 8.5997 -- time: 4.2665 sec.\n",
      "Step(2930) loss: 14.8192 -- time: 4.2057 sec.\n",
      "Step(2940) loss: 9.0029 -- time: 4.2759 sec.\n",
      "Step(2950) loss: 7.9471 -- time: 4.4485 sec.\n",
      "Step(2960) loss: 7.5883 -- time: 4.1918 sec.\n",
      "Step(2970) loss: 6.7209 -- time: 4.2583 sec.\n",
      "Step(2980) loss: 16.7665 -- time: 4.2627 sec.\n",
      "Step(2990) loss: 7.0969 -- time: 4.2848 sec.\n",
      "Step(3000) loss: 6.2431 -- time: 4.3359 sec.\n",
      "Step(3010) loss: 23.4470 -- time: 4.2634 sec.\n",
      "Step(3020) loss: 4.8478 -- time: 4.3230 sec.\n",
      "Step(3030) loss: 5.4391 -- time: 4.3710 sec.\n",
      "Step(3040) loss: 7.0431 -- time: 4.1986 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 2844.0960 - val_loss: 0.0000\n",
      "time: 78.4581 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 18/50\n",
      "----------------------------------------------------------------------\n",
      "Step(3050) loss: 17.2871 -- time: 3.1783 sec.\n",
      "Step(3060) loss: 12.9278 -- time: 4.3804 sec.\n",
      "Step(3070) loss: 14.1995 -- time: 4.2669 sec.\n",
      "Step(3080) loss: 8.3579 -- time: 4.2467 sec.\n",
      "Step(3090) loss: 7.3241 -- time: 4.3822 sec.\n",
      "Step(3100) loss: 7.5654 -- time: 4.3908 sec.\n",
      "Step(3110) loss: 23.5299 -- time: 4.2241 sec.\n",
      "Step(3120) loss: 16.4270 -- time: 4.2490 sec.\n",
      "Step(3130) loss: 4.3023 -- time: 4.2169 sec.\n",
      "Step(3140) loss: 5.2928 -- time: 4.3195 sec.\n",
      "Step(3150) loss: 8.3271 -- time: 4.3061 sec.\n",
      "Step(3160) loss: 61.4767 -- time: 4.3348 sec.\n",
      "Step(3170) loss: 20.3931 -- time: 4.3356 sec.\n",
      "Step(3180) loss: 8.6418 -- time: 4.3371 sec.\n",
      "Step(3190) loss: 7.7820 -- time: 4.2894 sec.\n",
      "Step(3200) loss: 13.5746 -- time: 4.4708 sec.\n",
      "Step(3210) loss: 19.5495 -- time: 4.3447 sec.\n",
      "Step(3220) loss: 26.0058 -- time: 4.2330 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 3231.6750 - val_loss: 0.0000\n",
      "time: 79.1958 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 19/50\n",
      "----------------------------------------------------------------------\n",
      "Step(3230) loss: 8.4425 -- time: 3.4584 sec.\n",
      "Step(3240) loss: 9.9988 -- time: 4.3547 sec.\n",
      "Step(3250) loss: 23.2898 -- time: 4.2266 sec.\n",
      "Step(3260) loss: 55.0803 -- time: 4.3517 sec.\n",
      "Step(3270) loss: 4.8094 -- time: 4.4622 sec.\n",
      "Step(3280) loss: 6.7326 -- time: 5.1627 sec.\n",
      "Step(3290) loss: 6.2656 -- time: 4.8917 sec.\n",
      "Step(3300) loss: 6.7476 -- time: 4.7684 sec.\n",
      "Step(3310) loss: 9.6987 -- time: 4.7248 sec.\n",
      "Step(3320) loss: 7.9672 -- time: 5.0042 sec.\n",
      "Step(3330) loss: 4.1017 -- time: 4.9957 sec.\n",
      "Step(3340) loss: 7.8170 -- time: 5.0044 sec.\n",
      "Step(3350) loss: 5.1318 -- time: 4.6638 sec.\n",
      "Step(3360) loss: 7.6409 -- time: 4.4040 sec.\n",
      "Step(3370) loss: 6.0499 -- time: 4.2323 sec.\n",
      "Step(3380) loss: 10.1005 -- time: 4.3157 sec.\n",
      "Step(3390) loss: 7.4342 -- time: 4.1820 sec.\n",
      "Step(3400) loss: 70.7190 -- time: 4.3060 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 3375.2055 - val_loss: 0.0000\n",
      "time: 83.6797 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 20/50\n",
      "----------------------------------------------------------------------\n",
      "Step(3410) loss: 29.8132 -- time: 3.8660 sec.\n",
      "Step(3420) loss: 75.6086 -- time: 4.3112 sec.\n",
      "Step(3430) loss: 19.1241 -- time: 4.4272 sec.\n",
      "Step(3440) loss: 71.6820 -- time: 4.3077 sec.\n",
      "Step(3450) loss: 6.0667 -- time: 4.4227 sec.\n",
      "Step(3460) loss: 8.0640 -- time: 4.5274 sec.\n",
      "Step(3470) loss: 6.8755 -- time: 4.4994 sec.\n",
      "Step(3480) loss: 8.5303 -- time: 4.3662 sec.\n",
      "Step(3490) loss: 7.5515 -- time: 4.3427 sec.\n",
      "Step(3500) loss: 8.3580 -- time: 4.2339 sec.\n",
      "Step(3510) loss: 4.5576 -- time: 4.3039 sec.\n",
      "Step(3520) loss: 8.1600 -- time: 4.3141 sec.\n",
      "Step(3530) loss: 6.8698 -- time: 4.2905 sec.\n",
      "Step(3540) loss: 7.5463 -- time: 4.2226 sec.\n",
      "Step(3550) loss: 7.6059 -- time: 4.1953 sec.\n",
      "Step(3560) loss: 5.7630 -- time: 4.3757 sec.\n",
      "Step(3570) loss: 43.6489 -- time: 4.2301 sec.\n",
      "Step(3580) loss: 12.5114 -- time: 4.1665 sec.\n",
      "----------------------------------------------------------------------\n",
      "(validation)\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 3495.5406 - val_loss: 2651.5069\n",
      "time: 112.9687 sec.\n",
      "--saved weights--\n",
      "----------------------------------------------------------------------\n",
      "Epoch 21/50\n",
      "----------------------------------------------------------------------\n",
      "Step(3590) loss: 6.0687 -- time: 4.4415 sec.\n",
      "Step(3600) loss: 6.8855 -- time: 4.1351 sec.\n",
      "Step(3610) loss: 8.1373 -- time: 4.3890 sec.\n",
      "Step(3620) loss: 90.9493 -- time: 4.2818 sec.\n",
      "Step(3630) loss: 17.0084 -- time: 4.4386 sec.\n",
      "Step(3640) loss: 7.2684 -- time: 4.4141 sec.\n",
      "Step(3650) loss: 106.3242 -- time: 4.4287 sec.\n",
      "Step(3660) loss: 4.5910 -- time: 4.2993 sec.\n",
      "Step(3670) loss: 6.3516 -- time: 4.5587 sec.\n",
      "Step(3680) loss: 6.5913 -- time: 4.3310 sec.\n",
      "Step(3690) loss: 31.7694 -- time: 4.3692 sec.\n",
      "Step(3700) loss: 30.6089 -- time: 4.3270 sec.\n",
      "Step(3710) loss: 8.9193 -- time: 4.3537 sec.\n",
      "Step(3720) loss: 7.5094 -- time: 4.2776 sec.\n",
      "Step(3730) loss: 73.4823 -- time: 4.3386 sec.\n",
      "Step(3740) loss: 6.1052 -- time: 4.2888 sec.\n",
      "Step(3750) loss: 7.4767 -- time: 4.2508 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 3439.7072 - val_loss: 0.0000\n",
      "time: 79.5548 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 22/50\n",
      "----------------------------------------------------------------------\n",
      "Step(3760) loss: 23.4806 -- time: 0.3487 sec.\n",
      "Step(3770) loss: 8.4203 -- time: 4.2123 sec.\n",
      "Step(3780) loss: 8.5444 -- time: 4.3341 sec.\n",
      "Step(3790) loss: 43.9350 -- time: 4.3841 sec.\n",
      "Step(3800) loss: 10.2178 -- time: 4.3996 sec.\n",
      "Step(3810) loss: 15.1506 -- time: 4.2980 sec.\n",
      "Step(3820) loss: 8.1049 -- time: 4.3718 sec.\n",
      "Step(3830) loss: 5.6784 -- time: 4.2781 sec.\n",
      "Step(3840) loss: 7.9142 -- time: 4.0972 sec.\n",
      "Step(3850) loss: 9.4661 -- time: 4.2449 sec.\n",
      "Step(3860) loss: 36.7466 -- time: 4.3242 sec.\n",
      "Step(3870) loss: 7.1250 -- time: 4.3315 sec.\n",
      "Step(3880) loss: 10.2885 -- time: 4.2730 sec.\n",
      "Step(3890) loss: 8.7543 -- time: 4.2904 sec.\n",
      "Step(3900) loss: 11.3034 -- time: 4.3078 sec.\n",
      "Step(3910) loss: 4.9927 -- time: 4.3247 sec.\n",
      "Step(3920) loss: 43.2750 -- time: 4.3480 sec.\n",
      "Step(3930) loss: 8.8377 -- time: 4.4269 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 3390.6622 - val_loss: 0.0000\n",
      "time: 78.9476 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 23/50\n",
      "----------------------------------------------------------------------\n",
      "Step(3940) loss: 6.8521 -- time: 0.7751 sec.\n",
      "Step(3950) loss: 133.4142 -- time: 4.3868 sec.\n",
      "Step(3960) loss: 5.9892 -- time: 4.4651 sec.\n",
      "Step(3970) loss: 4.0536 -- time: 4.3605 sec.\n",
      "Step(3980) loss: 43.9413 -- time: 4.4947 sec.\n",
      "Step(3990) loss: 4.5077 -- time: 4.2866 sec.\n",
      "Step(4000) loss: 7.8852 -- time: 4.2962 sec.\n",
      "Step(4010) loss: 7.6901 -- time: 4.3369 sec.\n",
      "Step(4020) loss: 50.6673 -- time: 4.3047 sec.\n",
      "Step(4030) loss: 35.1185 -- time: 4.2849 sec.\n",
      "Step(4040) loss: 5.8410 -- time: 4.2270 sec.\n",
      "Step(4050) loss: 20.3727 -- time: 4.1539 sec.\n",
      "Step(4060) loss: 6.2294 -- time: 4.2401 sec.\n",
      "Step(4070) loss: 8.0032 -- time: 4.3815 sec.\n",
      "Step(4080) loss: 70.3746 -- time: 4.3691 sec.\n",
      "Step(4090) loss: 55.5058 -- time: 4.3934 sec.\n",
      "Step(4100) loss: 15.3349 -- time: 4.2167 sec.\n",
      "Step(4110) loss: 6.4834 -- time: 4.1788 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 3152.8875 - val_loss: 0.0000\n",
      "time: 79.0846 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 24/50\n",
      "----------------------------------------------------------------------\n",
      "Step(4120) loss: 17.9549 -- time: 1.2063 sec.\n",
      "Step(4130) loss: 7.2926 -- time: 4.2760 sec.\n",
      "Step(4140) loss: 7.8489 -- time: 4.2351 sec.\n",
      "Step(4150) loss: 4.5178 -- time: 4.4952 sec.\n",
      "Step(4160) loss: 38.2998 -- time: 4.4470 sec.\n",
      "Step(4170) loss: 49.1659 -- time: 4.2824 sec.\n",
      "Step(4180) loss: 31.3977 -- time: 4.4440 sec.\n",
      "Step(4190) loss: 9.0073 -- time: 4.3129 sec.\n",
      "Step(4200) loss: 6.4273 -- time: 4.3174 sec.\n",
      "Step(4210) loss: 8.4544 -- time: 4.4452 sec.\n",
      "Step(4220) loss: 6.0957 -- time: 4.2867 sec.\n",
      "Step(4230) loss: 6.1761 -- time: 4.4766 sec.\n",
      "Step(4240) loss: 10.1599 -- time: 4.3218 sec.\n",
      "Step(4250) loss: 48.0977 -- time: 4.5360 sec.\n",
      "Step(4260) loss: 7.9589 -- time: 4.4547 sec.\n",
      "Step(4270) loss: 7.5527 -- time: 4.3600 sec.\n",
      "Step(4280) loss: 4.4891 -- time: 4.4362 sec.\n",
      "Step(4290) loss: 42.7365 -- time: 4.4167 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 3131.4382 - val_loss: 0.0000\n",
      "time: 80.2060 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 25/50\n",
      "----------------------------------------------------------------------\n",
      "Step(4300) loss: 4.8622 -- time: 1.6761 sec.\n",
      "Step(4310) loss: 8.0623 -- time: 4.4122 sec.\n",
      "Step(4320) loss: 8.1934 -- time: 4.4071 sec.\n",
      "Step(4330) loss: 20.4795 -- time: 4.4470 sec.\n",
      "Step(4340) loss: 8.4369 -- time: 4.3275 sec.\n",
      "Step(4350) loss: 7.9370 -- time: 4.3298 sec.\n",
      "Step(4360) loss: 70.9143 -- time: 4.4415 sec.\n",
      "Step(4370) loss: 4.4853 -- time: 4.4823 sec.\n",
      "Step(4380) loss: 45.9233 -- time: 4.4225 sec.\n",
      "Step(4390) loss: 6.0505 -- time: 4.6974 sec.\n",
      "Step(4400) loss: 7.5533 -- time: 4.4387 sec.\n",
      "Step(4410) loss: 8.1154 -- time: 4.3684 sec.\n",
      "Step(4420) loss: 12.4162 -- time: 4.7429 sec.\n",
      "Step(4430) loss: 6.3731 -- time: 4.5210 sec.\n",
      "Step(4440) loss: 58.9372 -- time: 4.3721 sec.\n",
      "Step(4450) loss: 25.4533 -- time: 4.4409 sec.\n",
      "Step(4460) loss: 7.0202 -- time: 4.3614 sec.\n",
      "Step(4470) loss: 7.7154 -- time: 4.2960 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 3237.3090 - val_loss: 0.0000\n",
      "time: 81.1334 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 26/50\n",
      "----------------------------------------------------------------------\n",
      "Step(4480) loss: 6.8871 -- time: 2.3120 sec.\n",
      "Step(4490) loss: 6.8233 -- time: 4.4430 sec.\n",
      "Step(4500) loss: 6.4390 -- time: 4.4806 sec.\n",
      "Step(4510) loss: 7.2174 -- time: 4.4273 sec.\n",
      "Step(4520) loss: 7.0870 -- time: 4.4371 sec.\n",
      "Step(4530) loss: 4.4113 -- time: 4.3013 sec.\n",
      "Step(4540) loss: 7.4137 -- time: 4.3315 sec.\n",
      "Step(4550) loss: 67.6911 -- time: 4.2486 sec.\n",
      "Step(4560) loss: 4.4366 -- time: 4.1897 sec.\n",
      "Step(4570) loss: 8.3186 -- time: 4.2877 sec.\n",
      "Step(4580) loss: 10.9600 -- time: 4.3195 sec.\n",
      "Step(4590) loss: 8.1195 -- time: 4.2762 sec.\n",
      "Step(4600) loss: 8.9693 -- time: 4.2584 sec.\n",
      "Step(4610) loss: 32.4065 -- time: 4.3350 sec.\n",
      "Step(4620) loss: 7.7712 -- time: 4.2592 sec.\n",
      "Step(4630) loss: 77.9403 -- time: 4.2659 sec.\n",
      "Step(4640) loss: 8.1229 -- time: 4.2531 sec.\n",
      "Step(4650) loss: 7.9631 -- time: 4.3559 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 2612.8113 - val_loss: 0.0000\n",
      "time: 79.3623 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 27/50\n",
      "----------------------------------------------------------------------\n",
      "Step(4660) loss: 8.8877 -- time: 2.5303 sec.\n",
      "Step(4670) loss: 7.3855 -- time: 4.2792 sec.\n",
      "Step(4680) loss: 16.4670 -- time: 4.4736 sec.\n",
      "Step(4690) loss: 7.3837 -- time: 4.1700 sec.\n",
      "Step(4700) loss: 7.2852 -- time: 4.3156 sec.\n",
      "Step(4710) loss: 9.9115 -- time: 4.2991 sec.\n",
      "Step(4720) loss: 7.1168 -- time: 4.5280 sec.\n",
      "Step(4730) loss: 12.8805 -- time: 4.2366 sec.\n",
      "Step(4740) loss: 7.8169 -- time: 4.3865 sec.\n",
      "Step(4750) loss: 7.5050 -- time: 4.3827 sec.\n",
      "Step(4760) loss: 114.4476 -- time: 4.5228 sec.\n",
      "Step(4770) loss: 53.6012 -- time: 4.3614 sec.\n",
      "Step(4780) loss: 4.6425 -- time: 4.3396 sec.\n",
      "Step(4790) loss: 74.2930 -- time: 4.2408 sec.\n",
      "Step(4800) loss: 5.0870 -- time: 4.2799 sec.\n",
      "Step(4810) loss: 26.5265 -- time: 4.3512 sec.\n",
      "Step(4820) loss: 152.7794 -- time: 4.3040 sec.\n",
      "Step(4830) loss: 21.1614 -- time: 4.3427 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 3331.7021 - val_loss: 0.0000\n",
      "time: 79.5092 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 28/50\n",
      "----------------------------------------------------------------------\n",
      "Step(4840) loss: 7.4351 -- time: 2.9941 sec.\n",
      "Step(4850) loss: 7.7418 -- time: 4.4087 sec.\n",
      "Step(4860) loss: 8.0511 -- time: 4.2913 sec.\n",
      "Step(4870) loss: 5.5676 -- time: 4.2204 sec.\n",
      "Step(4880) loss: 24.0826 -- time: 4.2385 sec.\n",
      "Step(4890) loss: 7.4551 -- time: 4.3218 sec.\n",
      "Step(4900) loss: 82.2894 -- time: 4.1702 sec.\n",
      "Step(4910) loss: 6.5449 -- time: 4.1587 sec.\n",
      "Step(4920) loss: 8.7426 -- time: 4.2495 sec.\n",
      "Step(4930) loss: 39.4988 -- time: 4.2742 sec.\n",
      "Step(4940) loss: 6.1312 -- time: 4.3997 sec.\n",
      "Step(4950) loss: 8.3664 -- time: 4.2689 sec.\n",
      "Step(4960) loss: 6.5621 -- time: 4.2817 sec.\n",
      "Step(4970) loss: 44.5719 -- time: 4.0955 sec.\n",
      "Step(4980) loss: 48.8580 -- time: 4.2507 sec.\n",
      "Step(4990) loss: 5.3188 -- time: 4.3060 sec.\n",
      "Step(5000) loss: 7.2924 -- time: 4.3653 sec.\n",
      "Step(5010) loss: 7.3958 -- time: 4.2299 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 3109.4276 - val_loss: 0.0000\n",
      "time: 78.1563 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 29/50\n",
      "----------------------------------------------------------------------\n",
      "Step(5020) loss: 7.7841 -- time: 3.4039 sec.\n",
      "Step(5030) loss: 6.9675 -- time: 4.3677 sec.\n",
      "Step(5040) loss: 7.7865 -- time: 4.4075 sec.\n",
      "Step(5050) loss: 4.5101 -- time: 4.3703 sec.\n",
      "Step(5060) loss: 5.4924 -- time: 4.1936 sec.\n",
      "Step(5070) loss: 8.3135 -- time: 4.3154 sec.\n",
      "Step(5080) loss: 56.4794 -- time: 4.3509 sec.\n",
      "Step(5090) loss: 35.5769 -- time: 4.2041 sec.\n",
      "Step(5100) loss: 39.3569 -- time: 4.4048 sec.\n",
      "Step(5110) loss: 9.6369 -- time: 4.2129 sec.\n",
      "Step(5120) loss: 7.3767 -- time: 4.2466 sec.\n",
      "Step(5130) loss: 26.7513 -- time: 4.3227 sec.\n",
      "Step(5140) loss: 6.6367 -- time: 4.3038 sec.\n",
      "Step(5150) loss: 10.2784 -- time: 4.4320 sec.\n",
      "Step(5160) loss: 5.9275 -- time: 4.2219 sec.\n",
      "Step(5170) loss: 34.7736 -- time: 4.3236 sec.\n",
      "Step(5180) loss: 7.4217 -- time: 4.2566 sec.\n",
      "Step(5190) loss: 6.5471 -- time: 4.2788 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 2915.3418 - val_loss: 0.0000\n",
      "time: 78.8175 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 30/50\n",
      "----------------------------------------------------------------------\n",
      "Step(5200) loss: 6.4493 -- time: 3.9393 sec.\n",
      "Step(5210) loss: 14.5485 -- time: 4.2854 sec.\n",
      "Step(5220) loss: 22.2698 -- time: 4.2754 sec.\n",
      "Step(5230) loss: 5.6459 -- time: 4.1361 sec.\n",
      "Step(5240) loss: 15.6874 -- time: 4.2552 sec.\n",
      "Step(5250) loss: 7.5134 -- time: 4.3617 sec.\n",
      "Step(5260) loss: 6.0384 -- time: 4.2722 sec.\n",
      "Step(5270) loss: 35.0109 -- time: 4.3830 sec.\n",
      "Step(5280) loss: 64.9426 -- time: 4.3713 sec.\n",
      "Step(5290) loss: 6.9271 -- time: 4.2256 sec.\n",
      "Step(5300) loss: 7.6686 -- time: 4.3326 sec.\n",
      "Step(5310) loss: 28.5759 -- time: 4.3255 sec.\n",
      "Step(5320) loss: 6.2090 -- time: 4.3218 sec.\n",
      "Step(5330) loss: 7.0779 -- time: 4.2978 sec.\n",
      "Step(5340) loss: 8.6572 -- time: 4.3226 sec.\n",
      "Step(5350) loss: 7.9908 -- time: 4.3184 sec.\n",
      "Step(5360) loss: 8.2453 -- time: 4.2128 sec.\n",
      "Step(5370) loss: 5.6313 -- time: 4.1975 sec.\n",
      "----------------------------------------------------------------------\n",
      "(validation)\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 3546.8952 - val_loss: 2588.0265\n",
      "time: 112.2219 sec.\n",
      "--saved weights--\n",
      "----------------------------------------------------------------------\n",
      "Epoch 31/50\n",
      "----------------------------------------------------------------------\n",
      "Step(5380) loss: 12.7638 -- time: 4.3899 sec.\n",
      "Step(5390) loss: 4.6052 -- time: 4.3985 sec.\n",
      "Step(5400) loss: 8.9447 -- time: 4.3214 sec.\n",
      "Step(5410) loss: 7.9333 -- time: 4.2677 sec.\n",
      "Step(5420) loss: 25.2535 -- time: 4.2929 sec.\n",
      "Step(5430) loss: 25.8250 -- time: 4.3827 sec.\n",
      "Step(5440) loss: 7.5594 -- time: 4.3088 sec.\n",
      "Step(5450) loss: 6.8016 -- time: 4.2386 sec.\n",
      "Step(5460) loss: 42.9070 -- time: 4.4197 sec.\n",
      "Step(5470) loss: 8.9071 -- time: 4.3603 sec.\n",
      "Step(5480) loss: 27.7740 -- time: 4.3896 sec.\n",
      "Step(5490) loss: 5.8868 -- time: 4.2334 sec.\n",
      "Step(5500) loss: 7.8773 -- time: 4.5161 sec.\n",
      "Step(5510) loss: 18.0253 -- time: 4.3753 sec.\n",
      "Step(5520) loss: 4.5985 -- time: 4.5814 sec.\n",
      "Step(5530) loss: 7.5873 -- time: 4.4881 sec.\n",
      "Step(5540) loss: 22.4584 -- time: 4.5531 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 3229.0224 - val_loss: 0.0000\n",
      "time: 80.1933 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 32/50\n",
      "----------------------------------------------------------------------\n",
      "Step(5550) loss: 7.8364 -- time: 0.3334 sec.\n",
      "Step(5560) loss: 8.2757 -- time: 4.2022 sec.\n",
      "Step(5570) loss: 7.1850 -- time: 4.3506 sec.\n",
      "Step(5580) loss: 8.7118 -- time: 4.4218 sec.\n",
      "Step(5590) loss: 7.3868 -- time: 4.3866 sec.\n",
      "Step(5600) loss: 6.3582 -- time: 4.3166 sec.\n",
      "Step(5610) loss: 7.3669 -- time: 4.3427 sec.\n",
      "Step(5620) loss: 17.2039 -- time: 4.3941 sec.\n",
      "Step(5630) loss: 71.4723 -- time: 4.2472 sec.\n",
      "Step(5640) loss: 5.1286 -- time: 4.3983 sec.\n",
      "Step(5650) loss: 8.7095 -- time: 4.3106 sec.\n",
      "Step(5660) loss: 63.6192 -- time: 4.3425 sec.\n",
      "Step(5670) loss: 8.8644 -- time: 4.1968 sec.\n",
      "Step(5680) loss: 6.7601 -- time: 4.3002 sec.\n",
      "Step(5690) loss: 8.1177 -- time: 4.1766 sec.\n",
      "Step(5700) loss: 6.8218 -- time: 4.2601 sec.\n",
      "Step(5710) loss: 20.3267 -- time: 4.2955 sec.\n",
      "Step(5720) loss: 8.3824 -- time: 4.3053 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 3318.4916 - val_loss: 0.0000\n",
      "time: 78.8708 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 33/50\n",
      "----------------------------------------------------------------------\n",
      "Step(5730) loss: 7.9590 -- time: 0.7808 sec.\n",
      "Step(5740) loss: 5.0786 -- time: 4.3413 sec.\n",
      "Step(5750) loss: 7.7677 -- time: 4.4071 sec.\n",
      "Step(5760) loss: 5.4677 -- time: 4.4817 sec.\n",
      "Step(5770) loss: 6.9691 -- time: 4.4770 sec.\n",
      "Step(5780) loss: 5.7000 -- time: 4.4676 sec.\n",
      "Step(5790) loss: 6.2676 -- time: 4.3482 sec.\n",
      "Step(5800) loss: 8.0299 -- time: 4.3505 sec.\n",
      "Step(5810) loss: 6.3432 -- time: 4.3262 sec.\n",
      "Step(5820) loss: 6.9523 -- time: 4.3755 sec.\n",
      "Step(5830) loss: 16.2414 -- time: 4.4453 sec.\n",
      "Step(5840) loss: 6.4157 -- time: 4.3853 sec.\n",
      "Step(5850) loss: 5.3050 -- time: 4.3456 sec.\n",
      "Step(5860) loss: 3.2966 -- time: 4.4551 sec.\n",
      "Step(5870) loss: 47.0458 -- time: 4.3294 sec.\n",
      "Step(5880) loss: 20.6806 -- time: 4.4677 sec.\n",
      "Step(5890) loss: 8.0936 -- time: 4.2830 sec.\n",
      "Step(5900) loss: 7.9341 -- time: 4.4846 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 2955.7955 - val_loss: 0.0000\n",
      "time: 80.5129 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 34/50\n",
      "----------------------------------------------------------------------\n",
      "Step(5910) loss: 7.0786 -- time: 1.3062 sec.\n",
      "Step(5920) loss: 4.5145 -- time: 4.3922 sec.\n",
      "Step(5930) loss: 6.3597 -- time: 4.4908 sec.\n",
      "Step(5940) loss: 7.0052 -- time: 4.4935 sec.\n",
      "Step(5950) loss: 7.6850 -- time: 4.3630 sec.\n",
      "Step(5960) loss: 6.4719 -- time: 4.3292 sec.\n",
      "Step(5970) loss: 36.2371 -- time: 4.3189 sec.\n",
      "Step(5980) loss: 8.0049 -- time: 4.4878 sec.\n",
      "Step(5990) loss: 45.9378 -- time: 4.6017 sec.\n",
      "Step(6000) loss: 79.1458 -- time: 4.3809 sec.\n",
      "Step(6010) loss: 7.4832 -- time: 4.4970 sec.\n",
      "Step(6020) loss: 17.3644 -- time: 4.2948 sec.\n",
      "Step(6030) loss: 5.8029 -- time: 4.4807 sec.\n",
      "Step(6040) loss: 17.6966 -- time: 4.5177 sec.\n",
      "Step(6050) loss: 6.5719 -- time: 4.6369 sec.\n",
      "Step(6060) loss: 6.5293 -- time: 4.4076 sec.\n",
      "Step(6070) loss: 5.4747 -- time: 4.4242 sec.\n",
      "Step(6080) loss: 6.8867 -- time: 4.3998 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 3682.8525 - val_loss: 0.0000\n",
      "time: 81.3353 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 35/50\n",
      "----------------------------------------------------------------------\n",
      "Step(6090) loss: 7.1993 -- time: 1.6601 sec.\n",
      "Step(6100) loss: 51.9181 -- time: 4.4369 sec.\n",
      "Step(6110) loss: 7.4494 -- time: 4.4081 sec.\n",
      "Step(6120) loss: 51.1676 -- time: 4.2641 sec.\n",
      "Step(6130) loss: 7.8447 -- time: 4.3818 sec.\n",
      "Step(6140) loss: 7.0739 -- time: 4.3485 sec.\n",
      "Step(6150) loss: 16.3981 -- time: 4.3514 sec.\n",
      "Step(6160) loss: 7.4474 -- time: 4.2395 sec.\n",
      "Step(6170) loss: 6.7156 -- time: 4.2123 sec.\n",
      "Step(6180) loss: 8.7668 -- time: 4.2771 sec.\n",
      "Step(6190) loss: 22.3428 -- time: 4.3134 sec.\n",
      "Step(6200) loss: 8.5734 -- time: 4.3743 sec.\n",
      "Step(6210) loss: 8.1252 -- time: 4.4444 sec.\n",
      "Step(6220) loss: 14.9853 -- time: 4.2655 sec.\n",
      "Step(6230) loss: 65.5131 -- time: 4.3613 sec.\n",
      "Step(6240) loss: 7.0709 -- time: 4.2566 sec.\n",
      "Step(6250) loss: 8.0359 -- time: 4.3746 sec.\n",
      "Step(6260) loss: 51.3738 -- time: 4.3600 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 3352.8209 - val_loss: 0.0000\n",
      "time: 79.3598 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 36/50\n",
      "----------------------------------------------------------------------\n",
      "Step(6270) loss: 4.4356 -- time: 2.0887 sec.\n",
      "Step(6280) loss: 19.1022 -- time: 4.1885 sec.\n",
      "Step(6290) loss: 6.1339 -- time: 4.2050 sec.\n",
      "Step(6300) loss: 7.6667 -- time: 4.2762 sec.\n",
      "Step(6310) loss: 32.3668 -- time: 4.3563 sec.\n",
      "Step(6320) loss: 8.4736 -- time: 4.2814 sec.\n",
      "Step(6330) loss: 14.8847 -- time: 4.2534 sec.\n",
      "Step(6340) loss: 8.7752 -- time: 4.1775 sec.\n",
      "Step(6350) loss: 7.6494 -- time: 4.5100 sec.\n",
      "Step(6360) loss: 7.2486 -- time: 4.3201 sec.\n",
      "Step(6370) loss: 55.6422 -- time: 4.2167 sec.\n",
      "Step(6380) loss: 4.7036 -- time: 4.3457 sec.\n",
      "Step(6390) loss: 6.7779 -- time: 4.3852 sec.\n",
      "Step(6400) loss: 8.5311 -- time: 4.3799 sec.\n",
      "Step(6410) loss: 72.2925 -- time: 4.2374 sec.\n",
      "Step(6420) loss: 7.1812 -- time: 4.3877 sec.\n",
      "Step(6430) loss: 17.2135 -- time: 4.3209 sec.\n",
      "Step(6440) loss: 56.9872 -- time: 4.4640 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 4041.7940 - val_loss: 0.0000\n",
      "time: 78.8698 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 37/50\n",
      "----------------------------------------------------------------------\n",
      "Step(6450) loss: 8.1012 -- time: 2.5501 sec.\n",
      "Step(6460) loss: 38.0483 -- time: 4.3217 sec.\n",
      "Step(6470) loss: 7.4689 -- time: 4.3215 sec.\n",
      "Step(6480) loss: 51.1825 -- time: 4.2911 sec.\n",
      "Step(6490) loss: 6.9010 -- time: 4.2285 sec.\n",
      "Step(6500) loss: 5.6139 -- time: 4.3033 sec.\n",
      "Step(6510) loss: 7.8048 -- time: 4.3675 sec.\n",
      "Step(6520) loss: 59.0388 -- time: 4.3474 sec.\n",
      "Step(6530) loss: 6.4543 -- time: 4.3714 sec.\n",
      "Step(6540) loss: 5.7996 -- time: 4.2949 sec.\n",
      "Step(6550) loss: 7.0976 -- time: 4.2123 sec.\n",
      "Step(6560) loss: 11.9872 -- time: 4.3929 sec.\n",
      "Step(6570) loss: 21.8674 -- time: 4.4375 sec.\n",
      "Step(6580) loss: 8.3196 -- time: 4.3741 sec.\n",
      "Step(6590) loss: 24.3230 -- time: 4.3294 sec.\n",
      "Step(6600) loss: 34.3502 -- time: 4.1725 sec.\n",
      "Step(6610) loss: 7.6032 -- time: 4.1801 sec.\n",
      "Step(6620) loss: 8.3556 -- time: 4.3373 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 4026.6127 - val_loss: 0.0000\n",
      "time: 78.8678 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 38/50\n",
      "----------------------------------------------------------------------\n",
      "Step(6630) loss: 7.9280 -- time: 3.1079 sec.\n",
      "Step(6640) loss: 4.6454 -- time: 4.4009 sec.\n",
      "Step(6650) loss: 64.8746 -- time: 4.3017 sec.\n",
      "Step(6660) loss: 25.5862 -- time: 4.2729 sec.\n",
      "Step(6670) loss: 8.4289 -- time: 4.2812 sec.\n",
      "Step(6680) loss: 8.2663 -- time: 4.3856 sec.\n",
      "Step(6690) loss: 6.4382 -- time: 4.3497 sec.\n",
      "Step(6700) loss: 44.9237 -- time: 4.4244 sec.\n",
      "Step(6710) loss: 6.6516 -- time: 4.3753 sec.\n",
      "Step(6720) loss: 12.2143 -- time: 4.3312 sec.\n",
      "Step(6730) loss: 6.2104 -- time: 4.3304 sec.\n",
      "Step(6740) loss: 13.8822 -- time: 4.3388 sec.\n",
      "Step(6750) loss: 25.9177 -- time: 4.2532 sec.\n",
      "Step(6760) loss: 7.1864 -- time: 4.2510 sec.\n",
      "Step(6770) loss: 8.0981 -- time: 4.2203 sec.\n",
      "Step(6780) loss: 7.4734 -- time: 4.3138 sec.\n",
      "Step(6790) loss: 9.2261 -- time: 4.3552 sec.\n",
      "Step(6800) loss: 22.5717 -- time: 4.3911 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 2951.2402 - val_loss: 0.0000\n",
      "time: 79.3338 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 39/50\n",
      "----------------------------------------------------------------------\n",
      "Step(6810) loss: 8.2581 -- time: 3.4208 sec.\n",
      "Step(6820) loss: 5.4050 -- time: 4.2488 sec.\n",
      "Step(6830) loss: 7.8386 -- time: 4.2077 sec.\n",
      "Step(6840) loss: 6.5208 -- time: 4.2954 sec.\n",
      "Step(6850) loss: 7.4891 -- time: 4.2992 sec.\n",
      "Step(6860) loss: 13.9420 -- time: 4.4389 sec.\n",
      "Step(6870) loss: 15.7730 -- time: 4.2849 sec.\n",
      "Step(6880) loss: 6.5932 -- time: 4.2807 sec.\n",
      "Step(6890) loss: 22.6461 -- time: 4.3171 sec.\n",
      "Step(6900) loss: 8.3064 -- time: 4.3577 sec.\n",
      "Step(6910) loss: 83.3539 -- time: 4.4064 sec.\n",
      "Step(6920) loss: 15.3291 -- time: 4.2948 sec.\n",
      "Step(6930) loss: 7.0257 -- time: 4.3188 sec.\n",
      "Step(6940) loss: 26.5476 -- time: 4.2677 sec.\n",
      "Step(6950) loss: 8.4626 -- time: 4.1529 sec.\n",
      "Step(6960) loss: 8.7567 -- time: 4.3620 sec.\n",
      "Step(6970) loss: 17.1179 -- time: 4.3997 sec.\n",
      "Step(6980) loss: 8.5773 -- time: 4.4310 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 3161.2610 - val_loss: 0.0000\n",
      "time: 78.9872 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 40/50\n",
      "----------------------------------------------------------------------\n",
      "Step(6990) loss: 6.5631 -- time: 3.8825 sec.\n",
      "Step(7000) loss: 8.3493 -- time: 4.3389 sec.\n",
      "Step(7010) loss: 7.7674 -- time: 4.4785 sec.\n",
      "Step(7020) loss: 6.9392 -- time: 4.3126 sec.\n",
      "Step(7030) loss: 7.0602 -- time: 4.4327 sec.\n",
      "Step(7040) loss: 7.0663 -- time: 4.4416 sec.\n",
      "Step(7050) loss: 12.8153 -- time: 4.2369 sec.\n",
      "Step(7060) loss: 5.5564 -- time: 4.3172 sec.\n",
      "Step(7070) loss: 31.8574 -- time: 4.3152 sec.\n",
      "Step(7080) loss: 4.6932 -- time: 4.2312 sec.\n",
      "Step(7090) loss: 8.2424 -- time: 4.2029 sec.\n",
      "Step(7100) loss: 59.9696 -- time: 4.4802 sec.\n",
      "Step(7110) loss: 68.3193 -- time: 4.4077 sec.\n",
      "Step(7120) loss: 10.1381 -- time: 4.3149 sec.\n",
      "Step(7130) loss: 8.9581 -- time: 4.3459 sec.\n",
      "Step(7140) loss: 7.8639 -- time: 4.3513 sec.\n",
      "Step(7150) loss: 76.1087 -- time: 4.3803 sec.\n",
      "Step(7160) loss: 6.8625 -- time: 4.1539 sec.\n",
      "----------------------------------------------------------------------\n",
      "(validation)\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 3430.5079 - val_loss: 2388.1489\n",
      "time: 112.5410 sec.\n",
      "--saved weights--\n",
      "----------------------------------------------------------------------\n",
      "Epoch 41/50\n",
      "----------------------------------------------------------------------\n",
      "Step(7170) loss: 6.5390 -- time: 4.3820 sec.\n",
      "Step(7180) loss: 5.3895 -- time: 4.2860 sec.\n",
      "Step(7190) loss: 7.3189 -- time: 4.1911 sec.\n",
      "Step(7200) loss: 8.5347 -- time: 4.4615 sec.\n",
      "Step(7210) loss: 4.1115 -- time: 4.3191 sec.\n",
      "Step(7220) loss: 4.0615 -- time: 4.3686 sec.\n",
      "Step(7230) loss: 20.4073 -- time: 4.2811 sec.\n",
      "Step(7240) loss: 7.0243 -- time: 4.2423 sec.\n",
      "Step(7250) loss: 6.1232 -- time: 4.2265 sec.\n",
      "Step(7260) loss: 4.0535 -- time: 4.2901 sec.\n",
      "Step(7270) loss: 7.3623 -- time: 4.4804 sec.\n",
      "Step(7280) loss: 26.9122 -- time: 4.3093 sec.\n",
      "Step(7290) loss: 9.2970 -- time: 4.4742 sec.\n",
      "Step(7300) loss: 100.0918 -- time: 4.3754 sec.\n",
      "Step(7310) loss: 9.4919 -- time: 4.4322 sec.\n",
      "Step(7320) loss: 6.3331 -- time: 4.4430 sec.\n",
      "Step(7330) loss: 6.0050 -- time: 4.5674 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 3539.0034 - val_loss: 0.0000\n",
      "time: 79.8085 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 42/50\n",
      "----------------------------------------------------------------------\n",
      "Step(7340) loss: 27.8908 -- time: 0.3511 sec.\n",
      "Step(7350) loss: 11.8390 -- time: 4.5033 sec.\n",
      "Step(7360) loss: 6.1661 -- time: 4.5424 sec.\n",
      "Step(7370) loss: 6.4194 -- time: 4.3794 sec.\n",
      "Step(7380) loss: 16.3521 -- time: 4.4733 sec.\n",
      "Step(7390) loss: 6.9150 -- time: 4.4210 sec.\n",
      "Step(7400) loss: 8.4861 -- time: 4.3682 sec.\n",
      "Step(7410) loss: 12.9727 -- time: 4.3542 sec.\n",
      "Step(7420) loss: 72.9100 -- time: 4.4068 sec.\n",
      "Step(7430) loss: 11.4131 -- time: 4.3980 sec.\n",
      "Step(7440) loss: 9.6580 -- time: 4.4645 sec.\n",
      "Step(7450) loss: 4.4341 -- time: 4.3152 sec.\n",
      "Step(7460) loss: 8.9124 -- time: 4.1920 sec.\n",
      "Step(7470) loss: 7.2201 -- time: 4.3360 sec.\n",
      "Step(7480) loss: 121.6920 -- time: 4.2691 sec.\n",
      "Step(7490) loss: 15.6777 -- time: 4.2659 sec.\n",
      "Step(7500) loss: 8.6399 -- time: 4.3104 sec.\n",
      "Step(7510) loss: 8.0868 -- time: 4.1801 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 3979.2527 - val_loss: 0.0000\n",
      "time: 79.8251 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 43/50\n",
      "----------------------------------------------------------------------\n",
      "Step(7520) loss: 9.8978 -- time: 0.7233 sec.\n",
      "Step(7530) loss: 39.1035 -- time: 4.1695 sec.\n",
      "Step(7540) loss: 8.0401 -- time: 4.2456 sec.\n",
      "Step(7550) loss: 8.3892 -- time: 4.2721 sec.\n",
      "Step(7560) loss: 5.8699 -- time: 4.3124 sec.\n",
      "Step(7570) loss: 4.6656 -- time: 4.2237 sec.\n",
      "Step(7580) loss: 7.3491 -- time: 4.3642 sec.\n",
      "Step(7590) loss: 20.8036 -- time: 4.2705 sec.\n",
      "Step(7600) loss: 79.7578 -- time: 4.3143 sec.\n",
      "Step(7610) loss: 6.3354 -- time: 4.3009 sec.\n",
      "Step(7620) loss: 8.2570 -- time: 4.3208 sec.\n",
      "Step(7630) loss: 38.3723 -- time: 4.2972 sec.\n",
      "Step(7640) loss: 63.7251 -- time: 4.3215 sec.\n",
      "Step(7650) loss: 7.0922 -- time: 4.2731 sec.\n",
      "Step(7660) loss: 4.1975 -- time: 4.2928 sec.\n",
      "Step(7670) loss: 7.1614 -- time: 4.3040 sec.\n",
      "Step(7680) loss: 27.9109 -- time: 4.1921 sec.\n",
      "Step(7690) loss: 5.4265 -- time: 4.3097 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 3367.2056 - val_loss: 0.0000\n",
      "time: 78.4482 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 44/50\n",
      "----------------------------------------------------------------------\n",
      "Step(7700) loss: 25.3900 -- time: 1.2222 sec.\n",
      "Step(7710) loss: 6.7824 -- time: 4.2860 sec.\n",
      "Step(7720) loss: 6.3692 -- time: 4.3905 sec.\n",
      "Step(7730) loss: 4.7136 -- time: 4.4714 sec.\n",
      "Step(7740) loss: 37.6566 -- time: 4.1726 sec.\n",
      "Step(7750) loss: 6.4831 -- time: 4.1835 sec.\n",
      "Step(7760) loss: 121.5547 -- time: 4.2634 sec.\n",
      "Step(7770) loss: 46.2817 -- time: 4.5052 sec.\n",
      "Step(7780) loss: 5.0391 -- time: 4.2203 sec.\n",
      "Step(7790) loss: 66.3211 -- time: 4.2246 sec.\n",
      "Step(7800) loss: 8.7231 -- time: 4.3958 sec.\n",
      "Step(7810) loss: 8.6931 -- time: 4.3616 sec.\n",
      "Step(7820) loss: 6.1699 -- time: 4.3161 sec.\n",
      "Step(7830) loss: 6.1196 -- time: 4.1240 sec.\n",
      "Step(7840) loss: 9.4919 -- time: 4.4345 sec.\n",
      "Step(7850) loss: 85.0426 -- time: 4.4377 sec.\n",
      "Step(7860) loss: 47.7674 -- time: 4.3295 sec.\n",
      "Step(7870) loss: 7.6350 -- time: 4.3536 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 3536.8132 - val_loss: 0.0000\n",
      "time: 79.0630 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 45/50\n",
      "----------------------------------------------------------------------\n",
      "Step(7880) loss: 36.3529 -- time: 1.6575 sec.\n",
      "Step(7890) loss: 8.3913 -- time: 4.3241 sec.\n",
      "Step(7900) loss: 59.8650 -- time: 4.2965 sec.\n",
      "Step(7910) loss: 6.5705 -- time: 4.3489 sec.\n",
      "Step(7920) loss: 39.0183 -- time: 4.2909 sec.\n",
      "Step(7930) loss: 8.5108 -- time: 4.3176 sec.\n",
      "Step(7940) loss: 6.1737 -- time: 4.2909 sec.\n",
      "Step(7950) loss: 5.1651 -- time: 4.3731 sec.\n",
      "Step(7960) loss: 5.8067 -- time: 4.1739 sec.\n",
      "Step(7970) loss: 6.6630 -- time: 4.3331 sec.\n",
      "Step(7980) loss: 10.7898 -- time: 4.3546 sec.\n",
      "Step(7990) loss: 35.5701 -- time: 4.3060 sec.\n",
      "Step(8000) loss: 7.6651 -- time: 4.4086 sec.\n",
      "Step(8010) loss: 14.9345 -- time: 4.2616 sec.\n",
      "Step(8020) loss: 8.2976 -- time: 4.3044 sec.\n",
      "Step(8030) loss: 64.6919 -- time: 4.2329 sec.\n",
      "Step(8040) loss: 8.9780 -- time: 4.4146 sec.\n",
      "Step(8050) loss: 13.5586 -- time: 4.2955 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 3552.4062 - val_loss: 0.0000\n",
      "time: 79.0310 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 46/50\n",
      "----------------------------------------------------------------------\n",
      "Step(8060) loss: 8.0749 -- time: 2.1495 sec.\n",
      "Step(8070) loss: 33.6887 -- time: 4.4164 sec.\n",
      "Step(8080) loss: 7.4720 -- time: 4.3564 sec.\n",
      "Step(8090) loss: 11.8429 -- time: 4.2346 sec.\n",
      "Step(8100) loss: 6.4105 -- time: 4.3593 sec.\n",
      "Step(8110) loss: 4.0675 -- time: 4.3625 sec.\n",
      "Step(8120) loss: 9.3246 -- time: 4.2721 sec.\n",
      "Step(8130) loss: 7.3723 -- time: 4.3235 sec.\n",
      "Step(8140) loss: 6.9271 -- time: 4.4816 sec.\n",
      "Step(8150) loss: 7.0920 -- time: 4.2196 sec.\n",
      "Step(8160) loss: 28.9269 -- time: 4.2551 sec.\n",
      "Step(8170) loss: 5.2882 -- time: 4.2592 sec.\n",
      "Step(8180) loss: 7.0940 -- time: 4.2990 sec.\n",
      "Step(8190) loss: 7.9829 -- time: 4.2137 sec.\n",
      "Step(8200) loss: 62.5547 -- time: 4.2602 sec.\n",
      "Step(8210) loss: 8.0259 -- time: 4.4372 sec.\n",
      "Step(8220) loss: 4.6911 -- time: 4.2925 sec.\n",
      "Step(8230) loss: 8.8483 -- time: 4.2584 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 3223.3872 - val_loss: 0.0000\n",
      "time: 79.0260 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 47/50\n",
      "----------------------------------------------------------------------\n",
      "Step(8240) loss: 6.6411 -- time: 2.5465 sec.\n",
      "Step(8250) loss: 23.2248 -- time: 4.4263 sec.\n",
      "Step(8260) loss: 8.0158 -- time: 4.2278 sec.\n",
      "Step(8270) loss: 53.8277 -- time: 4.3671 sec.\n",
      "Step(8280) loss: 21.8493 -- time: 4.2856 sec.\n",
      "Step(8290) loss: 38.9128 -- time: 4.2842 sec.\n",
      "Step(8300) loss: 5.8594 -- time: 4.2708 sec.\n",
      "Step(8310) loss: 4.2696 -- time: 4.4051 sec.\n",
      "Step(8320) loss: 53.9435 -- time: 4.4801 sec.\n",
      "Step(8330) loss: 4.8346 -- time: 4.2933 sec.\n",
      "Step(8340) loss: 7.3628 -- time: 4.3445 sec.\n",
      "Step(8350) loss: 49.3514 -- time: 4.3974 sec.\n",
      "Step(8360) loss: 137.7712 -- time: 4.3565 sec.\n",
      "Step(8370) loss: 9.1606 -- time: 4.3599 sec.\n",
      "Step(8380) loss: 6.9703 -- time: 4.2426 sec.\n",
      "Step(8390) loss: 6.6767 -- time: 4.3694 sec.\n",
      "Step(8400) loss: 5.9580 -- time: 4.3962 sec.\n",
      "Step(8410) loss: 39.3520 -- time: 4.4800 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 3497.7848 - val_loss: 0.0000\n",
      "time: 79.6357 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 48/50\n",
      "----------------------------------------------------------------------\n",
      "Step(8420) loss: 8.5309 -- time: 3.0068 sec.\n",
      "Step(8430) loss: 4.1454 -- time: 4.3486 sec.\n",
      "Step(8440) loss: 7.0685 -- time: 4.4556 sec.\n",
      "Step(8450) loss: 5.9327 -- time: 4.4752 sec.\n",
      "Step(8460) loss: 116.2736 -- time: 4.1769 sec.\n",
      "Step(8470) loss: 9.5276 -- time: 4.5159 sec.\n",
      "Step(8480) loss: 32.2861 -- time: 4.5014 sec.\n",
      "Step(8490) loss: 6.9614 -- time: 4.3746 sec.\n",
      "Step(8500) loss: 7.1428 -- time: 4.5004 sec.\n",
      "Step(8510) loss: 5.3838 -- time: 4.4537 sec.\n",
      "Step(8520) loss: 67.0821 -- time: 4.3563 sec.\n",
      "Step(8530) loss: 8.4847 -- time: 4.3279 sec.\n",
      "Step(8540) loss: 52.5207 -- time: 4.4323 sec.\n",
      "Step(8550) loss: 24.2418 -- time: 4.4639 sec.\n",
      "Step(8560) loss: 6.4631 -- time: 4.4621 sec.\n",
      "Step(8570) loss: 7.5224 -- time: 4.4440 sec.\n",
      "Step(8580) loss: 8.9860 -- time: 4.4252 sec.\n",
      "Step(8590) loss: 7.1889 -- time: 4.2222 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 3102.9610 - val_loss: 0.0000\n",
      "time: 80.6253 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 49/50\n",
      "----------------------------------------------------------------------\n",
      "Step(8600) loss: 7.4621 -- time: 3.4375 sec.\n",
      "Step(8610) loss: 7.2773 -- time: 4.3486 sec.\n",
      "Step(8620) loss: 7.3621 -- time: 4.4177 sec.\n",
      "Step(8630) loss: 7.4754 -- time: 4.6064 sec.\n",
      "Step(8640) loss: 16.4045 -- time: 4.2871 sec.\n",
      "Step(8650) loss: 27.7033 -- time: 4.3815 sec.\n",
      "Step(8660) loss: 92.2633 -- time: 4.3324 sec.\n",
      "Step(8670) loss: 7.9458 -- time: 4.3449 sec.\n",
      "Step(8680) loss: 5.9220 -- time: 4.3123 sec.\n",
      "Step(8690) loss: 8.2167 -- time: 4.2515 sec.\n",
      "Step(8700) loss: 6.8207 -- time: 4.4460 sec.\n",
      "Step(8710) loss: 6.2170 -- time: 4.6498 sec.\n",
      "Step(8720) loss: 6.8317 -- time: 4.8173 sec.\n",
      "Step(8730) loss: 6.3513 -- time: 4.5221 sec.\n",
      "Step(8740) loss: 7.8264 -- time: 4.5427 sec.\n",
      "Step(8750) loss: 8.5127 -- time: 4.4548 sec.\n",
      "Step(8760) loss: 8.8676 -- time: 4.6054 sec.\n",
      "Step(8770) loss: 8.4134 -- time: 4.6733 sec.\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 2389.3684 - val_loss: 0.0000\n",
      "time: 81.7235 sec.\n",
      "----------------------------------------------------------------------\n",
      "Epoch 50/50\n",
      "----------------------------------------------------------------------\n",
      "Step(8780) loss: 33.0086 -- time: 4.1075 sec.\n",
      "Step(8790) loss: 6.0378 -- time: 4.6559 sec.\n",
      "Step(8800) loss: 8.2400 -- time: 4.5063 sec.\n",
      "Step(8810) loss: 6.9361 -- time: 4.5166 sec.\n",
      "Step(8820) loss: 8.9201 -- time: 4.5375 sec.\n",
      "Step(8830) loss: 26.2898 -- time: 4.4957 sec.\n",
      "Step(8840) loss: 21.7581 -- time: 4.4888 sec.\n",
      "Step(8850) loss: 46.1521 -- time: 4.2440 sec.\n",
      "Step(8860) loss: 4.7580 -- time: 4.3541 sec.\n",
      "Step(8870) loss: 17.6014 -- time: 4.4054 sec.\n",
      "Step(8880) loss: 19.8887 -- time: 4.4521 sec.\n",
      "Step(8890) loss: 8.4040 -- time: 4.3382 sec.\n",
      "Step(8900) loss: 55.8995 -- time: 4.4435 sec.\n",
      "Step(8910) loss: 59.9365 -- time: 4.5450 sec.\n",
      "Step(8920) loss: 59.1374 -- time: 4.2947 sec.\n",
      "Step(8930) loss: 6.9047 -- time: 4.3923 sec.\n",
      "Step(8940) loss: 8.0458 -- time: 4.2855 sec.\n",
      "Step(8950) loss: 6.5234 -- time: 4.2137 sec.\n",
      "----------------------------------------------------------------------\n",
      "(validation)\n",
      "----------------------------------------------------------------------\n",
      "train_loss: 3968.1436 - val_loss: 2609.3578\n",
      "time: 114.7319 sec.\n",
      "--saved weights--\n",
      "CPU times: total: 28min\n",
      "Wall time: 1h 10min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "num_epochs = 50\n",
    "train(\n",
    "    net,\n",
    "    dataloaders_dict,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
